{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''General RL environment'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def advance(self, action):\n",
    "        '''\n",
    "        Moves one step in the environment.\n",
    "        Args:\n",
    "            action\n",
    "        Returns:\n",
    "            reward - double - reward\n",
    "            newState - int - new state\n",
    "            pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        return 0, 0, 0\n",
    "\n",
    "def make_riverSwim(epLen=20, nState=5):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "    states = {}\n",
    "    for s in range(nState):\n",
    "        states[(s)] = 0.0\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[0, 0] = (5/1000, 0)\n",
    "    R_true[nState - 1, 1] = (1, 0)\n",
    "\n",
    "    # Transitions\n",
    "    for s in range(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "    for s in range(1, nState - 1):\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 0.3\n",
    "        P_true[s, 1][s] = 0.6\n",
    "        P_true[s, 1][max(0, s-1)] = 0.1\n",
    "\n",
    "    P_true[0, 1][0] = 0.3\n",
    "    P_true[0, 1][1] = 0.7\n",
    "    P_true[nState - 1, 1][nState - 1] = 0.9\n",
    "    P_true[nState - 1, 1][nState - 2] = 0.1\n",
    "\n",
    "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
    "    riverSwim.R = R_true\n",
    "    riverSwim.P = P_true\n",
    "    riverSwim.states = states\n",
    "    riverSwim.reset()\n",
    "\n",
    "    return riverSwim\n",
    "\n",
    "class TabularMDP(Environment):\n",
    "    '''\n",
    "    Tabular MDP\n",
    "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
    "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nState, nAction, epLen):\n",
    "        '''\n",
    "        Initialize a tabular episodic MDP\n",
    "        Args:\n",
    "            nState  - int - number of states\n",
    "            nAction - int - number of actions\n",
    "            epLen   - int - episode length\n",
    "        Returns:\n",
    "            Environment object\n",
    "        '''\n",
    "\n",
    "        self.nState = nState\n",
    "        self.nAction = nAction\n",
    "        self.epLen = epLen\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "        # Now initialize R and P\n",
    "        self.R = {}\n",
    "        self.P = {}\n",
    "        self.states = {}\n",
    "        for state in range(nState):\n",
    "            for action in range(nAction):\n",
    "                self.R[state, action] = (1, 1)\n",
    "                self.P[state, action] = np.ones(nState) / nState\n",
    "\n",
    "    def reset(self):\n",
    "        \"Resets the Environment\"\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "    def advance(self,action):\n",
    "        '''\n",
    "        Move one step in the environment\n",
    "        Args:\n",
    "        action - int - chosen action\n",
    "        Returns:\n",
    "        reward - double - reward\n",
    "        newState - int - new state\n",
    "        episodeEnd - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        if self.R[self.state, action][1] < 1e-9:\n",
    "            # Hack for no noise\n",
    "            reward = self.R[self.state, action][0]\n",
    "        else:\n",
    "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
    "                                      scale=self.R[self.state, action][1])\n",
    "        #print(self.state, action, self.P[self.state, action])\n",
    "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
    "\n",
    "        # Update the environment\n",
    "        self.state = newState\n",
    "        self.timestep += 1\n",
    "\n",
    "        episodeEnd = 0\n",
    "        if self.timestep == self.epLen:\n",
    "            episodeEnd = 1\n",
    "            #newState = None\n",
    "            self.reset()\n",
    "\n",
    "        return reward, newState, episodeEnd\n",
    "\n",
    "    def argmax(self,b):\n",
    "        #print(b)\n",
    "        return np.random.choice(np.where(b == b.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UC_MatrixRL(object):\n",
    "    def __init__(self,env,N):\n",
    "        self.env = env\n",
    "        #See Step 3 of Algorithm 1\n",
    "        self.N = N\n",
    "        #The dimensionality of phi(s,a), see Step 2 or Assumption 1\n",
    "        self.d1 = self.env.nState * self.env.nAction\n",
    "        #The dimensionality of psi(s'), see Step 2 or Assumption 1\n",
    "        self.d2 = self.env.nState\n",
    "        #Step 2\n",
    "        self.features_state_action = {(s,a): np.zeros(self.d1) for s in self.env.states.keys() for a in range(self.env.nAction)}\n",
    "        #Step 3\n",
    "        self.features_next_state = {(s): np.zeros(self.d2) for s in self.env.states.keys()}\n",
    "        # A hack for using numpy's linear algebra functions in Step 4\n",
    "        self.features_next_state_mat = np.identity(self.d2)\n",
    "        # Creates the Identity Matrix for a dictionary\n",
    "        self.createIdentity()\n",
    "        # Initialize our Q matrix\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        #Step 4, this step maybe unnecessary since we are using Sherman-Morrison\n",
    "        self.A = np.identity(self.d1)\n",
    "        #For use in the Sherman-Morrison Update\n",
    "        self.Ainv = np.linalg.inv(self.A)\n",
    "        #Step 4\n",
    "        self.M = np.zeros((self.d1,self.d2))\n",
    "        #See Assumptions 2,2' and Theorem 1, this equals 1 in the tabular case\n",
    "        self.C_phi = 1.0\n",
    "        # See Assumption 2'(Stronger Feature Regularity), and consider the case when v_1 = v_2 = ....\n",
    "        self.C_psi = np.sqrt(env.nState)\n",
    "        # See Theorem 1\n",
    "        self.C_M = 1.0\n",
    "        # See Theorem 1\n",
    "        self.C_psi_ = 1.0\n",
    "        # This value scales our confidence interval, must be > 0\n",
    "        self.c = 1.0\n",
    "        # For use in updating M_n, see Step 13 and Eqn (2)\n",
    "        self.sums = np.zeros((self.d1,self.d2))\n",
    "        #Creates K_psi, see Section 3.1: Estimating the core matrix.\n",
    "        self.createK()\n",
    "        self.lam = 1.0\n",
    "        self.delta = 1/self.N\n",
    "\n",
    "    def createK(self):\n",
    "        '''\n",
    "        A function that creates K_psi and (K_psi)^-1 in computing M_n\n",
    "        See Section 3.1: Estimating the core matrix\n",
    "        '''\n",
    "        self.K = np.zeros((self.d2,self.d2))\n",
    "        for s_ in self.env.states.keys():\n",
    "            self.K = self.K + np.outer(self.features_next_state[s_],self.features_next_state[s_])\n",
    "        self.Kinv = np.linalg.inv(self.K)\n",
    "\n",
    "    def act(self,s,h):\n",
    "        '''\n",
    "        A function that returns the argmax of Q given the state and timestep\n",
    "        '''\n",
    "        return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def createIdentity(self):\n",
    "        '''\n",
    "            A function that creates the Identity Matrix for a Dictionary\n",
    "        '''\n",
    "        i = 0\n",
    "        for key in self.features_state_action.keys():\n",
    "            self.features_state_action[key][i] = 1\n",
    "            i += 1\n",
    "        j = 0\n",
    "        for key in self.features_next_state.keys():\n",
    "            self.features_next_state[key][j] = 1\n",
    "            j += 1\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def compute_Q(self,n):\n",
    "        '''\n",
    "        A function that computes the Optimisic Q-Values, see step 6 and Equations 4,8.\n",
    "        '''\n",
    "        Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        V = {h: np.zeros(self.env.nState) for h in range(self.env.epLen + 1)}\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    #For use in computing Q, like in UCRL_VTR we have access to the true reward function.\n",
    "                    r = self.env.R[s,a][0]\n",
    "\n",
    "                    value = np.dot(np.matmul(np.dot(self.features_state_action[s,a].T,self.M),\\\n",
    "                            self.features_next_state_mat),V[h+1])\n",
    "\n",
    "                    bonus = self.Beta(n) * np.dot(\\\n",
    "                           np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a])\n",
    "                    # Computing the optimistic Q-values as according to Eqn (8).\n",
    "                    Q[h,s,a] = self.proj(r+value+bonus,0,self.env.epLen)\n",
    "                V[h][s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        self.Q = Q.copy()\n",
    "        \n",
    "    '''\n",
    "    def Beta_2(self,n):\n",
    "        \n",
    "        A function that computes Beta under the assumption that Theorem 2 holds, see equation 8\n",
    "        with the multiplicitive H term included this is no longer an optimal bonus.\n",
    "        \n",
    "        first = self.c*(self.C_M * self.C_psi_ ** 2)\n",
    "        second = np.log(self.N*self.env.epLen*self.C_phi)*self.d1\n",
    "        #The line of code below for an 'anytime' version - there are no theoretical garanuntees for this statement.\n",
    "        #second = np.log(n*self.env.epLen*self.C_phi)*self.d1\n",
    "        return first * second\n",
    "    '''\n",
    "        \n",
    "    def Beta(self,n):\n",
    "        '''\n",
    "        A function that return Beta_k according to Theorem 20.5 in Bandits book\n",
    "        Also, if you return np.sqrt(first + second) instead of first + second then\n",
    "        you get higher cumlative reward. However, in Theorem 19.2, Beta_n is already\n",
    "        under the square root.\n",
    "        '''\n",
    "        #Step 3\n",
    "        #Bonus as according to step 3\n",
    "        '''\n",
    "        return np.sqrt(16*pow(self.m_2,2)*pow(env.epLen,2)*self.d*np.log(1+env.epLen*k) \\\n",
    "            *np.log(pow(k+1,2)*env.epLen/self.delta)*np.log(pow(k+1,2)*env.epLen/self.delta))\n",
    "        '''\n",
    "        \n",
    "        #Confidence bound from Chapter 20 of the Bandit Algorithms book, see Theorem 20.5.\n",
    "        first = np.sqrt(self.lam)*np.sqrt(self.C_M*self.d1)\n",
    "        (sign, logdet) = np.linalg.slogdet(self.A)\n",
    "        #second = np.sqrt(2*np.log(1/self.delta) + self.d*np.log((self.d*self.lam + k*self.L*self.L)/(self.d*self.lam)))\n",
    "        det = sign * logdet\n",
    "        second = np.sqrt(2*np.log(1/self.delta) + np.log(n) + min(det,pow(10,10)) - np.log(pow(self.lam,self.d1)))\n",
    "        #print(det)\n",
    "        return first + second\n",
    "\n",
    "    def update_core_matrix(self,s,a,s_):\n",
    "        '''\n",
    "        A function that performs step 12 and 13.\n",
    "        '''\n",
    "        #While the line below is in Algorithm 1, when using the Sherman Morrison update it is not needed.\n",
    "        #self.A = self.A + np.outer(self.features_state_action[s,a],self.features_state_action[s,a])\n",
    "        #Sherman Morrison Update (Rich's book Eqn 9.22)\n",
    "        self.Ainv = self.Ainv - np.dot((np.outer(np.dot(self.Ainv,self.features_state_action[s,a]) \\\n",
    "                 ,self.features_state_action[s,a])),self.Ainv) / \\\n",
    "                    (1 + np.dot(np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a]))\n",
    "        #The summation step of Eqn (2)\n",
    "        self.sums = self.sums + np.outer(self.features_state_action[s,a],self.features_next_state[s_])\n",
    "        #Here (K_psi)^-1 was pre-computed during the initialization. See Eqn (2)\n",
    "        self.M = np.matmul(np.matmul(self.Ainv,self.sums),self.Kinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_riverSwim(epLen = 20, nState = 4)\n",
    "N = 10000\n",
    "agent = UC_MatrixRL(env,N)\n",
    "R = 0\n",
    "Rvec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3a8bf37a3249948d6938beb858a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in tqdm(range(1,N+1)):\n",
    "    env.reset()\n",
    "    done = 0\n",
    "    while not done:\n",
    "        s = env.state\n",
    "        h = env.timestep\n",
    "        a = agent.act(s,h)\n",
    "        r,s_,done = env.advance(a)\n",
    "        R += r\n",
    "        agent.update_core_matrix(s,a,s_)\n",
    "    Rvec.append(R)\n",
    "    agent.compute_Q(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-norm of (True Transition Probabiltites - Estimated Probabilities) is: 0.11122891337039811\n"
     ]
    }
   ],
   "source": [
    "true_p = []\n",
    "for values in env.P.values():\n",
    "    for value in values:\n",
    "        true_p.append(value)\n",
    "        \n",
    "estimated_p = []\n",
    "for values in agent.M:\n",
    "    for value in values:\n",
    "        estimated_p.append(value)\n",
    "        \n",
    "print('The 2-norm of (True Transition Probabiltites - Estimated Probabilities) is:',np.linalg.norm(np.subtract(true_p,estimated_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.10502771666648"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(np.linalg.norm(agent.M),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2163ca90>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Rvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

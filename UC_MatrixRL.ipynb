{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''General RL environment'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def advance(self, action):\n",
    "        '''\n",
    "        Moves one step in the environment.\n",
    "        Args:\n",
    "            action\n",
    "        Returns:\n",
    "            reward - double - reward\n",
    "            newState - int - new state\n",
    "            pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        return 0, 0, 0\n",
    "\n",
    "def make_riverSwim(epLen=20, nState=5):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "    states = {}\n",
    "    for s in range(nState):\n",
    "        states[(s)] = 0.0\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[0, 0] = (5/1000, 0)\n",
    "    R_true[nState - 1, 1] = (1, 0)\n",
    "\n",
    "    # Transitions\n",
    "    for s in range(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "    for s in range(1, nState - 1):\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 0.3\n",
    "        P_true[s, 1][s] = 0.6\n",
    "        P_true[s, 1][max(0, s-1)] = 0.1\n",
    "\n",
    "    P_true[0, 1][0] = 0.3\n",
    "    P_true[0, 1][1] = 0.7\n",
    "    P_true[nState - 1, 1][nState - 1] = 0.9\n",
    "    P_true[nState - 1, 1][nState - 2] = 0.1\n",
    "\n",
    "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
    "    riverSwim.R = R_true\n",
    "    riverSwim.P = P_true\n",
    "    riverSwim.states = states\n",
    "    riverSwim.reset()\n",
    "\n",
    "    return riverSwim\n",
    "\n",
    "class TabularMDP(Environment):\n",
    "    '''\n",
    "    Tabular MDP\n",
    "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
    "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nState, nAction, epLen):\n",
    "        '''\n",
    "        Initialize a tabular episodic MDP\n",
    "        Args:\n",
    "            nState  - int - number of states\n",
    "            nAction - int - number of actions\n",
    "            epLen   - int - episode length\n",
    "        Returns:\n",
    "            Environment object\n",
    "        '''\n",
    "\n",
    "        self.nState = nState\n",
    "        self.nAction = nAction\n",
    "        self.epLen = epLen\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "        # Now initialize R and P\n",
    "        self.R = {}\n",
    "        self.P = {}\n",
    "        self.states = {}\n",
    "        for state in range(nState):\n",
    "            for action in range(nAction):\n",
    "                self.R[state, action] = (1, 1)\n",
    "                self.P[state, action] = np.ones(nState) / nState\n",
    "\n",
    "    def reset(self):\n",
    "        \"Resets the Environment\"\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "    def advance(self,action):\n",
    "        '''\n",
    "        Move one step in the environment\n",
    "        Args:\n",
    "        action - int - chosen action\n",
    "        Returns:\n",
    "        reward - double - reward\n",
    "        newState - int - new state\n",
    "        episodeEnd - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        if self.R[self.state, action][1] < 1e-9:\n",
    "            # Hack for no noise\n",
    "            reward = self.R[self.state, action][0]\n",
    "        else:\n",
    "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
    "                                      scale=self.R[self.state, action][1])\n",
    "        #print(self.state, action, self.P[self.state, action])\n",
    "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
    "\n",
    "        # Update the environment\n",
    "        self.state = newState\n",
    "        self.timestep += 1\n",
    "\n",
    "        episodeEnd = 0\n",
    "        if self.timestep == self.epLen:\n",
    "            episodeEnd = 1\n",
    "            #newState = None\n",
    "            self.reset()\n",
    "\n",
    "        return reward, newState, episodeEnd\n",
    "\n",
    "    def argmax(self,b):\n",
    "        #print(b)\n",
    "        return np.random.choice(np.where(b == b.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UC_MatrixRL(object):\n",
    "    def __init__(self,env,N):\n",
    "        self.env = env\n",
    "        #See Step 3 of Algorithm 1\n",
    "        self.N = N\n",
    "        #The dimensionality of phi(s,a), see Step 2 or Assumption 1\n",
    "        self.d1 = self.env.nState * self.env.nAction\n",
    "        #The dimensionality of psi(s'), see Step 2 or Assumption 1\n",
    "        self.d2 = self.env.nState\n",
    "        #Step 2\n",
    "        self.features_state_action = {(s,a): np.zeros(self.d1) for s in self.env.states.keys() for a in range(self.env.nAction)}\n",
    "        #Step 3\n",
    "        self.features_next_state = {(s): np.zeros(self.d2) for s in self.env.states.keys()}\n",
    "        # A hack for using numpy's linear algebra functions in Step 4\n",
    "        self.features_next_state_mat = np.identity(self.d2)\n",
    "        # Creates the Identity Matrix for a dictionary\n",
    "        self.createIdentity()\n",
    "        # Initialize our Q matrix\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        #Step 4\n",
    "        self.A = np.identity(self.d1)\n",
    "        #For use in the Sherman-Morrison Update\n",
    "        self.Ainv = np.linalg.inv(self.A)\n",
    "        #Step 4\n",
    "        self.M = np.zeros((self.d1,self.d2))\n",
    "        #See Assumptions 2,2' and Theorem 1, this equals 1 in the tabular case\n",
    "        self.C_phi = 1.0\n",
    "        # See Assumption 2'(Stronger Feature Regularity), and consider the case when v_1 = v_2 = ....\n",
    "        self.C_psi = np.sqrt(env.nState)\n",
    "        # See Theorem 1\n",
    "        self.C_M = 1.0\n",
    "        # See Theorem 1\n",
    "        self.C_psi_ = 1.0\n",
    "        # This value scales our confidence interval, must be > 0\n",
    "        self.c = 1.0\n",
    "        # For use in updating M_n, see Step 13 and Eqn (2)\n",
    "        self.sums = np.zeros((self.d1,self.d2))\n",
    "        #Creates K_psi, see Section 3.1: Estimating the core matrix.\n",
    "        self.createK()\n",
    "\n",
    "    def createK(self):\n",
    "        '''\n",
    "        A function that creates K_psi and (K_psi)^-1 for use in the Sherman-Morrison Update\n",
    "        See Section 3.1: Estimating the core matrix\n",
    "        '''\n",
    "        self.K = np.zeros((self.d2,self.d2))\n",
    "        for s_ in self.env.states.keys():\n",
    "            self.K = self.K + np.outer(self.features_next_state[s_],self.features_next_state[s_])\n",
    "        self.Kinv = np.linalg.inv(self.K)\n",
    "\n",
    "    def act(self,s,h):\n",
    "        '''\n",
    "        A function that returns the argmax of Q given the state and timestep\n",
    "        '''\n",
    "        return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def createIdentity(self):\n",
    "        '''\n",
    "            A function that creates the Identity Matrix for a Dictionary\n",
    "        '''\n",
    "        i = 0\n",
    "        for key in self.features_state_action.keys():\n",
    "            self.features_state_action[key][i] = 1\n",
    "            i += 1\n",
    "        j = 0\n",
    "        for key in self.features_next_state.keys():\n",
    "            self.features_next_state[key][j] = 1\n",
    "            j += 1\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def compute_Q(self,n):\n",
    "        '''\n",
    "        A function that computes the Optimisic Q-Values, see step 6 and Equations 4,8.\n",
    "        '''\n",
    "        Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        V = {h: np.zeros(self.env.nState) for h in range(self.env.epLen + 1)}\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    r = self.env.R[s,a][0]\n",
    "\n",
    "                    value = np.dot(np.matmul(np.dot(self.features_state_action[s,a].T,self.M),\\\n",
    "                            self.features_next_state_mat),V[h+1])\n",
    "\n",
    "                    bonus = 2 * self.C_psi * np.sqrt(self.Beta(n)) * np.dot(\\\n",
    "                            np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a])\n",
    "\n",
    "                    Q[h,s,a] = self.proj(r+value+bonus,0,self.env.epLen)\n",
    "                V[h][s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        self.Q = Q.copy()\n",
    "\n",
    "    def Beta(self,n):\n",
    "        '''\n",
    "        A function that computes Beta under the Assumption Theorem 2 holds, see equation 8\n",
    "        '''\n",
    "        first = self.c*(self.C_M * self.C_psi_ ** 2)\n",
    "        second = np.log(n*self.env.epLen*self.C_phi)*self.d1\n",
    "        return first * second\n",
    "\n",
    "    def update_core_matrix(self,s,a,s_):\n",
    "        '''\n",
    "        A function that performs step 12 and 13.\n",
    "        '''\n",
    "        self.A = self.A + np.outer(self.features_state_action[s,a],self.features_state_action[s,a])\n",
    "\n",
    "        self.Ainv = self.Ainv - np.dot((np.outer(np.dot(self.Ainv,self.features_state_action[s,a]) \\\n",
    "                 ,self.features_state_action[s,a])),self.Ainv) / \\\n",
    "                    (1 + np.dot(np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a]))\n",
    "\n",
    "        self.sums = self.sums + np.outer(self.features_state_action[s,a],self.features_next_state[s_])\n",
    "\n",
    "        self.M = np.matmul(np.matmul(self.Ainv,self.sums),self.Kinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_riverSwim(epLen = 20, nState = 4)\n",
    "N = 500\n",
    "agent = UC_MatrixRL(env,N)\n",
    "R = 0\n",
    "Rvec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a9da7c3eef4a02a733b6a9cf97ff14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in tqdm(range(1,N+1)):\n",
    "    env.reset()\n",
    "    done = 0\n",
    "    agent.compute_Q(n)\n",
    "    while not done:\n",
    "        s = env.state\n",
    "        h = env.timestep\n",
    "        a = agent.act(s,h)\n",
    "        r,s_,done = env.advance(a)\n",
    "        R += r\n",
    "        agent.update_core_matrix(s,a,s_)\n",
    "    Rvec.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-norm of (True Transition Probabiltites - Estimated Probabilities) is: 0.049446646085197006\n"
     ]
    }
   ],
   "source": [
    "true_p = []\n",
    "for values in env.P.values():\n",
    "    for value in values:\n",
    "        true_p.append(value)\n",
    "        \n",
    "estimated_p = []\n",
    "for values in agent.M:\n",
    "    for value in values:\n",
    "        estimated_p.append(value)\n",
    "        \n",
    "print('The 2-norm of (True Transition Probabiltites - Estimated Probabilities) is:',np.linalg.norm(np.subtract(true_p,estimated_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99618321, 0.        , 0.        , 0.        ],\n",
       "       [0.30895091, 0.69008662, 0.        , 0.        ],\n",
       "       [0.99421965, 0.        , 0.        , 0.        ],\n",
       "       [0.09371884, 0.57926221, 0.32652044, 0.        ],\n",
       "       [0.        , 0.99295775, 0.        , 0.        ],\n",
       "       [0.        , 0.09533989, 0.60709705, 0.29713553],\n",
       "       [0.        , 0.        , 0.97142857, 0.        ],\n",
       "       [0.        , 0.        , 0.10692921, 0.89282154]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

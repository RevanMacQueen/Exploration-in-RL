{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import bernoulli\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''General RL environment'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def advance(self, action):\n",
    "        '''\n",
    "        Moves one step in the environment.\n",
    "        Args:\n",
    "            action\n",
    "        Returns:\n",
    "            reward - double - reward\n",
    "            newState - int - new state\n",
    "            pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        return 0, 0, 0\n",
    "\n",
    "class Opt_Policy(object):\n",
    "    '''\n",
    "    For Computing Q*, the optimal q_values. This code was written with RiverSwim in mind.\n",
    "    '''\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros((self.env.epLen+1,self.env.nState,self.env.nAction))\n",
    "        self.V = np.zeros((self.env.epLen+1,self.env.nState))\n",
    "    \n",
    "    def update(self):\n",
    "        Q = np.zeros((self.env.epLen + 1,self.env.nState,self.env.nAction))\n",
    "        V = np.zeros((self.env.epLen + 1,self.env.nState))\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in range(self.env.nState):\n",
    "                for a in range(self.env.nAction):\n",
    "                    reward = env.R[s,a][0]\n",
    "                    '''\n",
    "                    for s_ in range(self.env.nState):\n",
    "                        #print(s,a,s_)\n",
    "                        reward += self.env.R[s,a,s_][0]*self.env.P[s,a][s_]\n",
    "                    '''\n",
    "                    p = env.P[s,a]\n",
    "                    Q[h,s,a] = reward + np.inner(p,V[h+1,:])\n",
    "                V[h,s] = max(Q[h,s,:])\n",
    "        self.Q = Q.copy()\n",
    "        self.V = V.copy()\n",
    "    \n",
    "    def act(self,s,h):\n",
    "        return env.argmax(self.Q[h,s,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_riverSwim(epLen=20, nState=5):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "    states = {}\n",
    "    for s in range(nState):\n",
    "        states[(s)] = 0.0\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[0, 0] = (5/1000, 0)\n",
    "    R_true[nState - 1, 1] = (1, 0)\n",
    "\n",
    "    # Transitions\n",
    "    for s in range(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "    for s in range(1, nState - 1):\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 0.3\n",
    "        P_true[s, 1][s] = 0.6\n",
    "        P_true[s, 1][max(0, s-1)] = 0.1\n",
    "\n",
    "    P_true[0, 1][0] = 0.3\n",
    "    P_true[0, 1][1] = 0.7\n",
    "    P_true[nState - 1, 1][nState - 1] = 0.9\n",
    "    P_true[nState - 1, 1][nState - 2] = 0.1\n",
    "\n",
    "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
    "    riverSwim.R = R_true\n",
    "    riverSwim.P = P_true\n",
    "    riverSwim.states = states\n",
    "    riverSwim.reset()\n",
    "\n",
    "    return riverSwim\n",
    "\n",
    "def make_MDP(epLen=2,nBottom = 24):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nState = nBottom + 3\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "    states = {}\n",
    "    for s in range(nState):\n",
    "        states[(s)] = 0.0\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[1,0] = (0, 0)\n",
    "    R_true[1,1] = (0, 0)\n",
    "    R_true[2,0] = (1, 0)\n",
    "    R_true[2,1] = (1, 0)\n",
    "    n = int(nBottom/4)\n",
    "    #Transitions \n",
    "    P_true[0,0][1] = 1.0\n",
    "    P_true[0,1][2] = 1.0\n",
    "    P_true[1,0][3:3+n] = 1/n\n",
    "    P_true[1,1][3+n:3+2*n] = 1/n\n",
    "    P_true[2,0][3+2*n:3+3*n] = 1/n\n",
    "    P_true[2,1][3+3*n:3+4*n] = 1/n\n",
    "    \n",
    "\n",
    "    MDP = TabularMDP(nState, nAction, epLen)\n",
    "    MDP.R = R_true\n",
    "    MDP.P = P_true\n",
    "    MDP.states = states\n",
    "    MDP.reset()\n",
    "\n",
    "    return MDP\n",
    "\n",
    "class TabularMDP(Environment):\n",
    "    '''\n",
    "    Tabular MDP\n",
    "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
    "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nState, nAction, epLen):\n",
    "        '''\n",
    "        Initialize a tabular episodic MDP\n",
    "        Args:\n",
    "            nState  - int - number of states\n",
    "            nAction - int - number of actions\n",
    "            epLen   - int - episode length\n",
    "        Returns:\n",
    "            Environment object\n",
    "        '''\n",
    "\n",
    "        self.nState = nState\n",
    "        self.nAction = nAction\n",
    "        self.epLen = epLen\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "        # Now initialize R and P\n",
    "        self.R = {}\n",
    "        self.P = {}\n",
    "        self.states = {}\n",
    "        for state in range(nState):\n",
    "            for action in range(nAction):\n",
    "                self.R[state, action] = (1, 1)\n",
    "                self.P[state, action] = np.ones(nState) / nState\n",
    "                \n",
    "    def reset(self):\n",
    "        \"Resets the Environment\"\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "        \n",
    "    def advance(self,action):\n",
    "        '''\n",
    "        Move one step in the environment\n",
    "        Args:\n",
    "        action - int - chosen action\n",
    "        Returns:\n",
    "        reward - double - reward\n",
    "        newState - int - new state\n",
    "        episodeEnd - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        if self.R[self.state, action][1] < 1e-9:\n",
    "            # Hack for no noise\n",
    "            reward = self.R[self.state, action][0]\n",
    "        else:\n",
    "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
    "                                      scale=self.R[self.state, action][1])\n",
    "        #print(self.state, action, self.P[self.state, action])\n",
    "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
    "        \n",
    "        # Update the environment\n",
    "        self.state = newState\n",
    "        self.timestep += 1\n",
    "\n",
    "        episodeEnd = 0\n",
    "        if self.timestep == self.epLen:\n",
    "            episodeEnd = 1\n",
    "            #newState = None\n",
    "            self.reset()\n",
    "\n",
    "        return reward, newState, episodeEnd\n",
    "    \n",
    "    def argmax(self,b):\n",
    "        #print(b)\n",
    "        return np.random.choice(np.where(b == b.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCRL_VTR(object):\n",
    "    '''\n",
    "    Algorithm 1 as described in the paper Model-Based RL with\n",
    "    Value-Target Regression\n",
    "    The algorithm assumes that the rewards are in the [0,1] interval.\n",
    "    '''\n",
    "    def __init__(self,env,K):\n",
    "        self.env = env\n",
    "        self.K = K\n",
    "        # A unit test that randomly explores for a period of time then learns from that experience\n",
    "        # Here self.random_explore is a way to select a period of random exploration.\n",
    "        # When the current episode k > total number of episodes K divided by self.random_explore\n",
    "        # the algorithm switches to the greedy action with respect to its action value Q(s,a).\n",
    "        # Here the dimension (self.d) for the Tabular setting is |S x A x S| as stated in Appendix B\n",
    "        self.d = self.env.nState * self.env.nAction * self.env.nState\n",
    "        # In the tabular setting the basis models is just the dxd identity matrix, see Appendix B\n",
    "        self.P_basis = np.identity(self.d)\n",
    "        #Our Q-values are initialized as a 2d numpy array, will eventually convert to a dictionary\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        #Our State Value function is initialized as a 1d numpy error, will eventually convert to a dictionary\n",
    "        self.V = {(h,s): 0.0 for s in self.env.states.keys() for h in range(env.epLen + 1)} # self.V[env.epLen] stays zero\n",
    "        #self.create_value_functions()\n",
    "        #The index of each (s,a,s') tuple, see Appendix B\n",
    "        self.sigma = {}\n",
    "        self.state_idx = {}\n",
    "        self.createIdx()\n",
    "        #See Step 2, of algorithm 1\n",
    "#         self.M = env.epLen**2*self.d*np.identity(self.d)\n",
    "        # For use in the confidence bound bonus term, see Beta function down below\n",
    "        self.lam = 1.0\n",
    "        #Self.L is no longer need, but will keep for now.\n",
    "        self.L = 1.0\n",
    "        self.M = np.identity(self.d)*self.lam\n",
    "        self.Minv = np.identity(self.d)*(1/self.lam)\n",
    "        #See Step 2\n",
    "        self.w = np.zeros(self.d)\n",
    "        #See Step 2\n",
    "        self.theta = np.dot(self.Minv,self.w)\n",
    "        #See Step 3\n",
    "        self.delta = 1/self.K\n",
    "        #m_2 >= the 2-norm of theta_star, see Bandit Algorithms Theorem 20.5\n",
    "        #self.error()\n",
    "        #self.m_2 = np.linalg.norm(self.true_p) + 0.1\n",
    "        self.m_2 = np.sqrt(self.env.nState*self.env.nAction)\n",
    "        self.d1 = env.nState * env.nAction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def feature_vector(self,s,a,h):\n",
    "        '''\n",
    "        Returning sum_{s'} V[h+1][s'] P_dot(s'|s,a),\n",
    "        with V stored in self.\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        sums = np.zeros(self.d)\n",
    "        for s_ in self.env.states.keys():\n",
    "            #print(s,s_)\n",
    "            sums += self.V[h+1,s_] * self.P_basis[self.sigma[(s,a,s_)]]\n",
    "        return sums\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def update_Q(self,s,a,k,h):\n",
    "        '''\n",
    "        A function that updates both Q and V, Q is updated according to equation 4 and\n",
    "        V is updated according to equation 2\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            k - the current episode\n",
    "            h - the current timestep within the episode\n",
    "        Currently, does not properly compute the Q-values but it does seem to learn theta_star\n",
    "        '''\n",
    "        #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "        # Alex's code: X = self.X[h,:]\n",
    "        # Suggested code:\n",
    "        X = self.feature_vector(s,a,h)\n",
    "        self.Q[h,s,a] = self.proj(self.env.R[(s,a)][0] + np.dot(X,self.theta) + self.Beta(h) \\\n",
    "            * np.sqrt(np.dot(np.dot(np.transpose(X),self.Minv),X)), 0, self.env.epLen)\n",
    "        self.V[h,s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def update_Qend(self,k):\n",
    "        '''\n",
    "        A function that updates both Q and V at the end of each episode, see step 16 of algorithm 1\n",
    "        Inputs:\n",
    "            k - the current episode\n",
    "        '''\n",
    "        #step 16\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "                    # Alex's code: X = self.X[h,:]\n",
    "                    # Suggested code:\n",
    "                    self.update_Q(s,a,k,h)\n",
    "                self.V[h,s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def update_stat(self,s,a,s_,h):\n",
    "        '''\n",
    "        A function that performs steps 9-13 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            a - the action\n",
    "            s_ - the next state\n",
    "            k - the current episode\n",
    "            h - the timestep within episode when s was visited (starting at zero)\n",
    "        '''\n",
    "        #Step 10\n",
    "#         self.X[h,:] = self.feature_vector(s,a,h) # do not need to store this\n",
    "        X = self.feature_vector(s,a,h)\n",
    "        #Step 11\n",
    "        y = self.V[h+1,s_]\n",
    "#         if s_ != None:\n",
    "#             y = self.V[h+1][s_]\n",
    "#         else:\n",
    "#             y = 0.0\n",
    "        #Step 12\n",
    "        self.M = self.M + np.outer(X,X)\n",
    "        self.Minv = self.Minv - np.dot((np.outer(np.dot(self.Minv,X),X)),self.Minv) / \\\n",
    "                    (1 + np.dot(np.dot(X,self.Minv),X))\n",
    "        #Step 13\n",
    "        self.w = self.w + y*X\n",
    "\n",
    "    def update_param(self):\n",
    "        '''\n",
    "        Updates our approximation of theta_star at the end of each episode, see\n",
    "        Step 15 of algorithm1\n",
    "        '''\n",
    "        #Step 15\n",
    "        self.theta = np.matmul(self.Minv,self.w)\n",
    "\n",
    "    def act(self,s,h,k):\n",
    "        '''\n",
    "        Returns the greedy action with respect to Q_{h,k}(s,a) for a \\in A\n",
    "        see step 8 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        \n",
    "        return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        \n",
    "\n",
    "    def createIdx(self):\n",
    "        '''\n",
    "        A simple function that creates sigma according to Appendix B.\n",
    "        Here sigma is a dictionary who inputs is a tuple (s,a,s') and stores\n",
    "        the interger index to be used in our basis model P.\n",
    "        '''\n",
    "        i = 0\n",
    "        j = 0\n",
    "        k = 0\n",
    "        for s in self.env.states.keys():\n",
    "            self.state_idx[s] = int(j)\n",
    "            j += 1\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in self.env.states.keys():\n",
    "                    self.sigma[(s,a,s_)] = int(i)\n",
    "                    i += 1\n",
    "\n",
    "    def Beta(self,h):\n",
    "        '''\n",
    "        A function that return Beta_k according to Algorithm 1, step 3\n",
    "        '''\n",
    "        #Step 3\n",
    "        #Confidence bound from Appendix F/Chapter 20 of Bandit Algorithms Chpt 20 (Lattimore/Szepesvari).\n",
    "        first = np.sqrt(self.lam)*self.m_2\n",
    "        (sign, logdet) = np.linalg.slogdet(self.M)\n",
    "        det = sign * logdet\n",
    "        second = (self.env.epLen-h)/2*np.sqrt(2*np.log(1/self.delta) + min(det,pow(10,10)) - np.log(pow(self.lam,self.d)))\n",
    "        return first + second\n",
    "    \n",
    "    def getweightedL1(self):\n",
    "        self.weights = self.count\n",
    "        self.true_p = np.zeros((self.env.nState,self.env.nAction,self.env.nState))\n",
    "        for s in range(self.env.nState):\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in range(self.env.nState):\n",
    "                    self.true_p[s,a,s_] = self.env.P[s,a][s_]\n",
    "                    \n",
    "                    #for numerical stability\n",
    "                    if sum(self.count[s,a,:]) == 0:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/1.0\n",
    "                        \n",
    "                    else:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/sum(self.count[s,a,:])\n",
    "                        \n",
    "        self.weights = self.weights.reshape(env.nState*env.nAction*env.nState)\n",
    "        self.true_p = self.true_p.reshape(env.nState*env.nAction*env.nState)\n",
    "        temp = 0\n",
    "        for i in range(env.nState*env.nAction*env.nState):\n",
    "            temp += abs(self.theta[i]-self.true_p[i])*self.weights[i]\n",
    "        return temp\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Simulates the agent interacting with the environment over K episodes\n",
    "        Input: Nothing\n",
    "        Output: A Kx1 reward vector and a Kx1 model error vector\n",
    "        '''\n",
    "        R = []\n",
    "        reward = 0.0\n",
    "        #Stores counts for use in weighted L1 norm\n",
    "        self.count = np.zeros((env.nState,env.nAction,env.nState))\n",
    "        self.model_error = np.zeros(self.K)\n",
    "        print(self.name())\n",
    "        for k in tqdm(range(1,self.K+1)):\n",
    "            self.env.reset()\n",
    "            done = 0\n",
    "            while done != 1:\n",
    "                s = self.env.state\n",
    "                h = self.env.timestep\n",
    "                a = self.act(s,h,k)\n",
    "                r,s_,done = self.env.advance(a)\n",
    "                self.count[s,a,s_] += 1\n",
    "                reward += r\n",
    "                self.update_stat(s,a,s_,h)\n",
    "            self.update_param()\n",
    "            self.update_Qend(k)\n",
    "            R.append(reward)\n",
    "            self.model_error[k-1] = self.getweightedL1()\n",
    "        return R\n",
    "\n",
    "    def name(self):\n",
    "        return 'Running: UCRL_VTR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EG_MatrixRL(object):\n",
    "    def __init__(self,env,N,eps):\n",
    "        self.env = env\n",
    "        #See Step 3 of Algorithm 1\n",
    "        self.N = N\n",
    "        self.eps = eps\n",
    "        #The dimensionality of phi(s,a), see Step 2 or Assumption 1\n",
    "        self.d1 = self.env.nState * self.env.nAction\n",
    "        #The dimensionality of psi(s'), see Step 2 or Assumption 1\n",
    "        self.d2 = self.env.nState\n",
    "        #Step 2\n",
    "        self.features_state_action = {(s,a): np.zeros(self.d1) for s in self.env.states.keys() for a in range(self.env.nAction)}\n",
    "        #Step 3\n",
    "        self.features_next_state = {(s): np.zeros(self.d2) for s in self.env.states.keys()}\n",
    "        # A hack for using numpy's linear algebra functions in Step 4\n",
    "        self.features_next_state_mat = np.identity(self.d2)\n",
    "        # Creates the Identity Matrix for a dictionary\n",
    "        self.createIdentity()\n",
    "        # Initialize our Q matrix\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        #Step 4, this step maybe unnecessary since we are using Sherman-Morrison\n",
    "        self.A = np.identity(self.d1)\n",
    "        #For use in the Sherman-Morrison Update\n",
    "        self.Ainv = np.linalg.inv(self.A)\n",
    "        #Step 4\n",
    "        self.M = np.zeros((self.d1,self.d2))\n",
    "        self.M_mat = np.zeros((self.N+1,self.d1*self.d2))\n",
    "        self.M_mat[0,:] = self.M.reshape(self.d1*self.d2)\n",
    "        #See Assumptions 2,2' and Theorem 1, this equals 1 in the tabular case\n",
    "        self.C_phi = 1.0\n",
    "        # See Assumption 2'(Stronger Feature Regularity), and consider the case when v_1 = v_2 = ....\n",
    "        self.C_psi = np.sqrt(env.nState)\n",
    "        # See Theorem 1\n",
    "        self.C_M = 1.0\n",
    "        # See Theorem 1\n",
    "        self.C_psi_ = 1.0\n",
    "        # This value scales our confidence interval, must be > 0\n",
    "        self.c = 1.0\n",
    "        # For use in updating M_n, see Step 13 and Eqn (2)\n",
    "        self.sums = np.zeros((self.d1,self.d2))\n",
    "        #Creates K_psi, see Section 3.1: Estimating the core matrix.\n",
    "        self.createK()\n",
    "        self.lam = 1.0\n",
    "        self.delta = 1/self.N\n",
    "\n",
    "\n",
    "\n",
    "    def createK(self):\n",
    "        '''\n",
    "        A function that creates K_psi and (K_psi)^-1 in computing M_n\n",
    "        '''\n",
    "        self.K = np.zeros((self.d2,self.d2))\n",
    "        for s_ in self.env.states.keys():\n",
    "            self.K = self.K + np.outer(self.features_next_state[s_],self.features_next_state[s_])\n",
    "        self.Kinv = np.linalg.inv(self.K)\n",
    "\n",
    "    def act(self,s,h):\n",
    "        '''\n",
    "        Returns the eps-greedy action with respect to Q_{h,k}(s,a) for a \\in A\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        \n",
    "        c = np.random.uniform()\n",
    "        if c >= self.eps:\n",
    "            return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        else:\n",
    "            return np.random.choice([0,1])\n",
    "        \n",
    "\n",
    "    def createIdentity(self):\n",
    "        '''\n",
    "        A function that creates the Identity Matrix for a Dictionary\n",
    "        '''\n",
    "        i = 0\n",
    "        for key in self.features_state_action.keys():\n",
    "            self.features_state_action[key][i] = 1\n",
    "            i += 1\n",
    "        j = 0\n",
    "        for key in self.features_next_state.keys():\n",
    "            self.features_next_state[key][j] = 1\n",
    "            j += 1\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def compute_Q(self,n):\n",
    "        '''\n",
    "        A function that computes the Optimisic Q-Values.\n",
    "        '''\n",
    "        Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        V = {h: np.zeros(self.env.nState) for h in range(self.env.epLen + 1)}\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    #For use in computing Q, like in UCRL_VTR we have access to the true reward function.\n",
    "                    r = self.env.R[s,a][0]\n",
    "\n",
    "                    value = np.dot(np.matmul(np.dot(self.features_state_action[s,a].T,self.M),\\\n",
    "                            self.features_next_state_mat),V[h+1])\n",
    "                    Q[h,s,a] = self.proj(r+value,0,self.env.epLen)\n",
    "                V[h][s] = (1-self.eps/2)*max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)])) \\\n",
    "                        + (self.eps/2)*min(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        self.Q = Q.copy()\n",
    "\n",
    "        \n",
    "\n",
    "    def update_core_matrix(self,s,a,s_):\n",
    "        \n",
    "        #Sherman Morrison Update (Rich's book Eqn 9.22)\n",
    "        self.Ainv = self.Ainv - np.dot((np.outer(np.dot(self.Ainv,self.features_state_action[s,a]) \\\n",
    "                 ,self.features_state_action[s,a])),self.Ainv) / \\\n",
    "                    (1 + np.dot(np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a]))\n",
    "        \n",
    "        self.sums = self.sums + np.outer(self.features_state_action[s,a],self.features_next_state[s_])\n",
    "        #Here (K_psi)^-1 was pre-computed during the initialization. \n",
    "        self.M = np.matmul(np.matmul(self.Ainv,self.sums),self.Kinv)\n",
    "        \n",
    "\n",
    "    \n",
    "    def name(self):\n",
    "        return 'Running: EG_MatrixRL'\n",
    "    \n",
    "    def getweightedL1(self):\n",
    "        '''\n",
    "        Returns the weighed L1 norm of the current estimated model and the true environment model\n",
    "        '''\n",
    "        self.weights = self.count\n",
    "        self.true_p = np.zeros((self.env.nState,self.env.nAction,self.env.nState))\n",
    "        for s in range(self.env.nState):\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in range(self.env.nState):\n",
    "                    self.true_p[s,a,s_] = self.env.P[s,a][s_]\n",
    "                    \n",
    "                    #for numerical stability\n",
    "                    if sum(self.count[s,a,:]) == 0:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/1.0\n",
    "                        \n",
    "                    else:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/sum(self.count[s,a,:])\n",
    "                        \n",
    "        self.weights = self.weights.reshape(env.nState*env.nAction*env.nState)\n",
    "        self.true_p = self.true_p.reshape(env.nState*env.nAction*env.nState)\n",
    "        self.theta = self.M.reshape(env.nState*env.nAction*env.nState)\n",
    "        temp = 0\n",
    "        for i in range(env.nState*env.nAction*env.nState):\n",
    "            temp += abs(self.theta[i]-self.true_p[i])*self.weights[i]\n",
    "        return temp\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Simulates the agent interacting with the environment over K episodes\n",
    "        Input: Nothing\n",
    "        Output: A Kx1 reward vector and a Kx1 model error vector\n",
    "        '''\n",
    "        R = 0\n",
    "        Rvec = []\n",
    "        Reg = []\n",
    "        #Stores count to be used in Weighted L1 Norm\n",
    "        self.count = np.zeros((env.nState,env.nAction,env.nState))\n",
    "        self.model_error = np.zeros(self.N)\n",
    "        #print(self.name())\n",
    "        for n in (range(1,self.N+1)):\n",
    "            self.env.reset()\n",
    "            done = 0\n",
    "            self.compute_Q(n)\n",
    "            while not done:\n",
    "                s = self.env.state\n",
    "                h = self.env.timestep\n",
    "                a = self.act(s,h)\n",
    "                r,s_,done = self.env.advance(a)\n",
    "                self.count[s,a,s_] += 1\n",
    "                R += r\n",
    "                self.update_core_matrix(s,a,s_)\n",
    "            Rvec.append(R)\n",
    "            self.model_error[n-1] = self.getweightedL1()\n",
    "        return Rvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UC_MatrixRL(object):\n",
    "    def __init__(self,env,N):\n",
    "        self.env = env\n",
    "        #See Step 3 of Algorithm 1\n",
    "        self.N = N\n",
    "        #The dimensionality of phi(s,a), see Step 2 or Assumption 1\n",
    "        self.d1 = self.env.nState * self.env.nAction\n",
    "        #The dimensionality of psi(s'), see Step 2 or Assumption 1\n",
    "        self.d2 = self.env.nState\n",
    "        #Step 2\n",
    "        self.features_state_action = {(s,a): np.zeros(self.d1) for s in self.env.states.keys() for a in range(self.env.nAction)}\n",
    "        #Step 3\n",
    "        self.features_next_state = {(s): np.zeros(self.d2) for s in self.env.states.keys()}\n",
    "        # A hack for using numpy's linear algebra functions in Step 4\n",
    "        self.features_next_state_mat = np.identity(self.d2)\n",
    "        # Creates the Identity Matrix for a dictionary\n",
    "        self.createIdentity()\n",
    "        # Initialize our Q matrix\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        #Step 4, this step may be unnecessary since we are using Sherman-Morrison\n",
    "        self.A = np.identity(self.d1)\n",
    "        #For use in the Sherman-Morrison Update\n",
    "        self.Ainv = np.linalg.inv(self.A)\n",
    "        #Step 4\n",
    "        self.M = np.zeros((self.d1,self.d2))\n",
    "        #See Assumptions 2,2' and Theorem 1, this equals 1 in the tabular case\n",
    "        self.C_phi = 1.0\n",
    "        # See Assumption 2'(Stronger Feature Regularity), and consider the case when v_1 = v_2 = ....\n",
    "        self.C_psi = 1.0\n",
    "        # See Theorem 1\n",
    "        self.C_M = 1.0\n",
    "        # See Theorem 1\n",
    "        self.C_psi_ = 1.0\n",
    "        # This value scales our confidence interval, must be > 0\n",
    "        self.c = 1.0\n",
    "        # For use in updating M_n, see Step 13 and Eqn (2)\n",
    "        self.sums = np.zeros((self.d1,self.d2))\n",
    "        #Creates K_psi.\n",
    "        self.createK()\n",
    "        self.lam = 1.0\n",
    "        self.delta = 1/self.N\n",
    "\n",
    "    def createK(self):\n",
    "        '''\n",
    "        A function that creates K_psi and (K_psi)^-1 in computing M_n\n",
    "        '''\n",
    "        \n",
    "        self.K = np.zeros((self.d2,self.d2))\n",
    "        for s_ in self.env.states.keys():\n",
    "            self.K = self.K + np.outer(self.features_next_state[s_],self.features_next_state[s_])\n",
    "        self.Kinv = np.linalg.inv(self.K)\n",
    "\n",
    "    def act(self,s,h):\n",
    "        '''\n",
    "        A function that returns the argmax of Q given the state and timestep\n",
    "        '''\n",
    "        return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def createIdentity(self):\n",
    "        '''\n",
    "            A function that creates the Identity Matrix for a Dictionary\n",
    "        '''\n",
    "        i = 0\n",
    "        for key in self.features_state_action.keys():\n",
    "            self.features_state_action[key][i] = 1\n",
    "            i += 1\n",
    "        j = 0\n",
    "        for key in self.features_next_state.keys():\n",
    "            self.features_next_state[key][j] = 1\n",
    "            j += 1\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def compute_Q(self,n):\n",
    "        '''\n",
    "        A function that computes the Optimisic Q-Values, see step 6 and Equations 4,8.\n",
    "        '''\n",
    "        Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        V = {h: np.zeros(self.env.nState) for h in range(self.env.epLen + 1)}\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    #For use in computing Q, like in UCRL_VTR we have access to the true reward function.\n",
    "                    r = self.env.R[s,a][0]\n",
    "\n",
    "                    value = np.dot(np.matmul(np.dot(self.features_state_action[s,a].T,self.M),\\\n",
    "                            self.features_next_state_mat),V[h+1])\n",
    "                    \n",
    "                    bonus = 2 * self.C_psi * self.env.epLen * self.Beta(h) * np.sqrt(np.dot(\\\n",
    "                           np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a]))\n",
    "                    # Computing the optimistic Q-values as according to Eqn (8).\n",
    "                    Q[h,s,a] = self.proj(r+value+bonus,0,self.env.epLen)\n",
    "                V[h][s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        self.Q = Q.copy()\n",
    "\n",
    "\n",
    "    def Beta(self,h):\n",
    "        '''\n",
    "        A function that return Beta_k according to Theorem 20.5 in Bandits Algorithms (Lattimore/Szepesvari)/\n",
    "        Appendix F of the VTR paper.\n",
    "        Log Determinate was used for numerical stability purposes\n",
    "        '''\n",
    "        #Confidence bound from Appendix F of the our VTR-paper.\n",
    "        first = np.sqrt(self.lam)*np.sqrt(self.C_M*self.d1)\n",
    "        (sign, logdet) = np.linalg.slogdet(scipy.linalg.sqrtm(self.A))\n",
    "        det = sign * logdet\n",
    "        second = (self.env.epLen - h)/2*np.sqrt(2*np.log(1/self.delta) + min(det,pow(10,10)) - np.log(pow(self.lam,self.d1)))\n",
    "        #print(det)\n",
    "        #print(first+second)\n",
    "        return first + second\n",
    "\n",
    "    def update_core_matrix(self,s,a,s_):\n",
    "        '''\n",
    "        A function that performs step 12 and 13.\n",
    "        '''\n",
    "        #While the line below is in Algorithm 1, when using the Sherman Morrison update it is not needed.\n",
    "        #self.A = self.A + np.outer(self.features_state_action[s,a],self.features_state_action[s,a])\n",
    "        #Sherman Morrison Update (Rich's book Eqn 9.22)\n",
    "        self.Ainv = self.Ainv - np.dot((np.outer(np.dot(self.Ainv,self.features_state_action[s,a]) \\\n",
    "                 ,self.features_state_action[s,a])),self.Ainv) / \\\n",
    "                    (1 + np.dot(np.dot(self.features_state_action[s,a],self.Ainv),self.features_state_action[s,a]))\n",
    "        #The summation step of Eqn (2)\n",
    "        self.sums = self.sums + np.outer(self.features_state_action[s,a],self.features_next_state[s_])\n",
    "        #Here (K_psi)^-1 was pre-computed during the initialization. See Eqn (2)\n",
    "        self.M = np.matmul(np.matmul(self.Ainv,self.sums),self.Kinv)\n",
    "\n",
    "    def name(self):\n",
    "        return 'Running: UC_MatrixRL'\n",
    "    \n",
    "    def getweightedL1(self):\n",
    "        '''\n",
    "        Returns the weighed L1 norm of the current estimated model and the true environment model\n",
    "        '''\n",
    "        self.weights = self.count\n",
    "        self.true_p = np.zeros((self.env.nState,self.env.nAction,self.env.nState))\n",
    "        for s in range(self.env.nState):\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in range(self.env.nState):\n",
    "                    self.true_p[s,a,s_] = self.env.P[s,a][s_]\n",
    "                    \n",
    "                    #for numerical stability\n",
    "                    if sum(self.count[s,a,:]) == 0:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/1.0\n",
    "                        \n",
    "                    else:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/sum(self.count[s,a,:])\n",
    "                        \n",
    "        self.weights = self.weights.reshape(env.nState*env.nAction*env.nState)\n",
    "        self.true_p = self.true_p.reshape(env.nState*env.nAction*env.nState)\n",
    "        self.theta = self.M.reshape(env.nState*env.nAction*env.nState)\n",
    "        temp = 0\n",
    "        for i in range(env.nState*env.nAction*env.nState):\n",
    "            temp += abs(self.theta[i]-self.true_p[i])*self.weights[i]\n",
    "        return temp\n",
    "            \n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Simulates the agent interacting with the environment over K episodes\n",
    "        Input: Nothing\n",
    "        Output: A Kx1 reward vector and a Kx1 model error vector\n",
    "        '''\n",
    "        R = 0\n",
    "        Rvec = []\n",
    "        #Stores count to be used in Weighted L1 Norm\n",
    "        self.count = np.zeros((env.nState,env.nAction,env.nState))\n",
    "        self.model_error = np.zeros(self.N)\n",
    "        print(self.name())\n",
    "        for n in tqdm(range(1,self.N+1)):\n",
    "            self.env.reset()\n",
    "            done = 0\n",
    "            self.compute_Q(n)\n",
    "            while not done:\n",
    "                s = self.env.state\n",
    "                h = self.env.timestep\n",
    "                a = self.act(s,h)\n",
    "                r,s_,done = self.env.advance(a)\n",
    "                self.count[s,a,s_] += 1\n",
    "                R += r\n",
    "                self.update_core_matrix(s,a,s_)\n",
    "            Rvec.append(R)\n",
    "            self.model_error[n-1] = self.getweightedL1()\n",
    "        return Rvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGRL_VTR(object):\n",
    "    '''\n",
    "    Eps-Greedy VTR as described in the paper Model-Based RL with\n",
    "    Value-Target Regression\n",
    "    The algorithm assumes that the rewards are in the [0,1] interval.\n",
    "    '''\n",
    "    def __init__(self,env,K,eps):\n",
    "        self.env = env\n",
    "        self.K = K\n",
    "        self.epsilon = eps\n",
    "        # Here the dimension (self.d) for the Tabular setting is |S x A x S|\n",
    "        self.d = self.env.nState * self.env.nAction * self.env.nState\n",
    "        # In the tabular setting the basis models is just the dxd identity matrix\n",
    "        self.P_basis = np.identity(self.d)\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        self.V = {(h,s): 0.0 for s in self.env.states.keys() for h in range(env.epLen + 1)} # self.V[env.epLen] stays zero\n",
    "        self.sigma = {}\n",
    "        self.state_idx = {}\n",
    "        self.createIdx()\n",
    "        self.lam = 1.0\n",
    "        #Self.L is no longer need, but will keep for now.\n",
    "        self.L = 1.0\n",
    "        self.M = np.identity(self.d)*self.lam\n",
    "        self.Minv = np.identity(self.d)*(1/self.lam)\n",
    "        self.w = np.zeros(self.d)\n",
    "        self.theta = np.dot(self.Minv,self.w)\n",
    "        self.delta = 1/self.K\n",
    "        self.m_2 = np.sqrt(self.env.nState*self.env.nAction)\n",
    "        self.d1 = env.nState * env.nAction\n",
    "\n",
    "    def feature_vector(self,s,a,h):\n",
    "        '''\n",
    "        Returning sum_{s'} V[h+1][s'] P_dot(s'|s,a),\n",
    "        with V stored in self.\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        sums = np.zeros(self.d)\n",
    "        for s_ in self.env.states.keys():\n",
    "            #print(s,s_)\n",
    "            sums += self.V[h+1,s_] * self.P_basis[self.sigma[(s,a,s_)]]\n",
    "        return sums\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def update_Q(self,s,a,k,h):\n",
    "        '''\n",
    "        A function that updates both Q and V, Q is updated according to equation 4 and\n",
    "        V is updated according to equation 2\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            k - the current episode\n",
    "            h - the current timestep within the episode\n",
    "        Currently, does not properly compute the Q-values but it does seem to learn theta_star\n",
    "        '''\n",
    "        #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "       \n",
    "        X = self.feature_vector(s,a,h)\n",
    "        self.Q[h,s,a] = self.proj(self.env.R[(s,a)][0] + np.dot(X,self.theta), 0, self.env.epLen)\n",
    "        self.V[h,s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def update_Qend(self,k):\n",
    "        '''\n",
    "        Updates the Value functions at the end of each episode.\n",
    "        '''\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "                    \n",
    "                    self.update_Q(s,a,k,h)\n",
    "                self.V[h,s] = (1 - self.epsilon/2) * max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)])) \\\n",
    "                                + self.epsilon/2 * min(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def update_stat(self,s,a,s_,h):\n",
    "        '''\n",
    "        A function that performs steps 9-13 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            a - the action\n",
    "            s_ - the next state\n",
    "            k - the current episode\n",
    "            h - the timestep within episode when s was visited (starting at zero)\n",
    "        '''\n",
    "        \n",
    "        X = self.feature_vector(s,a,h)\n",
    "    \n",
    "        y = self.V[h+1,s_]\n",
    "        \n",
    "        self.M = self.M + np.outer(X,X)\n",
    "        #Sherman-Morrison Update for M^{-1}, saves signficant computation\n",
    "        self.Minv = self.Minv - np.dot((np.outer(np.dot(self.Minv,X),X)),self.Minv) / \\\n",
    "                    (1 + np.dot(np.dot(X,self.Minv),X))\n",
    "        \n",
    "        self.w = self.w + y*X\n",
    "\n",
    "    def update_param(self):\n",
    "        '''\n",
    "        Updates our approximation of theta_star at the end of each episode\n",
    "        '''\n",
    "        #Step 15\n",
    "        self.theta = np.matmul(self.Minv,self.w)\n",
    "\n",
    "    def act(self,s,h,k):\n",
    "        '''\n",
    "        Returns the eps-greedy action with respect to Q_{h,k}(s,a) for a \\in A\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        c = np.random.uniform()\n",
    "        if c < self.epsilon:\n",
    "            return np.random.choice(np.array([0,1]))\n",
    "        else:\n",
    "            return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        \n",
    "\n",
    "    def createIdx(self):\n",
    "        '''\n",
    "        A simple function that creates sigma according to Appendix B.\n",
    "        Here sigma is a dictionary who inputs is a tuple (s,a,s') and stores\n",
    "        the interger index to be used in our basis model P.\n",
    "        '''\n",
    "        i = 0\n",
    "        j = 0\n",
    "        k = 0\n",
    "        for s in self.env.states.keys():\n",
    "            self.state_idx[s] = int(j)\n",
    "            j += 1\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in self.env.states.keys():\n",
    "                    self.sigma[(s,a,s_)] = int(i)\n",
    "                    i += 1\n",
    "    \n",
    "    def getweightedL1(self):\n",
    "        '''\n",
    "        Returns the weighed L1 norm of the current estimated model and the true environment model\n",
    "        '''\n",
    "        self.weights = self.count\n",
    "        self.true_p = np.zeros((self.env.nState,self.env.nAction,self.env.nState))\n",
    "        for s in range(self.env.nState):\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in range(self.env.nState):\n",
    "                    self.true_p[s,a,s_] = self.env.P[s,a][s_]\n",
    "                    \n",
    "                    #for numerical stability\n",
    "                    if sum(self.count[s,a,:]) == 0:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/1.0\n",
    "                        \n",
    "                    else:\n",
    "                        self.weights[s,a,s_] = self.count[s,a,s_]/sum(self.count[s,a,:])\n",
    "                        \n",
    "        self.weights = self.weights.reshape(env.nState*env.nAction*env.nState)\n",
    "        self.true_p = self.true_p.reshape(env.nState*env.nAction*env.nState)\n",
    "        temp = 0\n",
    "        for i in range(env.nState*env.nAction*env.nState):\n",
    "            temp += abs(self.theta[i]-self.true_p[i])*self.weights[i]\n",
    "        return temp\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Simulates the agent interacting with the environment over K episodes\n",
    "        Input: Nothing\n",
    "        Output: A Kx1 reward vector and a Kx1 model error vector\n",
    "        '''\n",
    "        R = []\n",
    "        reward = 0.0\n",
    "        #print(self.name())\n",
    "        #Stores count to be used in Weighted L1 Norm\n",
    "        self.count = np.zeros((env.nState,env.nAction,env.nState))\n",
    "        self.model_error = np.zeros(self.K)\n",
    "        for k in (range(1,self.K+1)):\n",
    "            self.env.reset()\n",
    "            done = 0\n",
    "            while done != 1:\n",
    "                s = self.env.state\n",
    "                h = self.env.timestep\n",
    "                a = self.act(s,h,k)\n",
    "                r,s_,done = self.env.advance(a)\n",
    "                self.count[s,a,s_] += 1\n",
    "                reward += r\n",
    "                self.update_stat(s,a,s_,h)\n",
    "            self.update_param()\n",
    "            self.update_Qend(k)\n",
    "            R.append(reward)\n",
    "            self.model_error[k-1] = self.getweightedL1()\n",
    "        return R\n",
    "\n",
    "    def name(self):\n",
    "        '''\n",
    "        A function that prints the name of the Algorithm\n",
    "        This function is called in the run(self) function\n",
    "        ''' \n",
    "        return 'Running: EGRL_VTR'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a137b284fd248e192d6e8b044e57887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: UCRL_VTR\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e340020d925146d3a85bbf7f64147fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: UC_MatrixRL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b6564f8a0b4194b93cf288de31b7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30932ff00df44d13ac2c14ad1a95c492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: UCRL_VTR\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096556fd683f4c7b9c92e01229a1e64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: UC_MatrixRL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e591296bec49419337a6de9e7afe18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab4bb6f4e44470985356ba4fdf7d856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: UCRL_VTR\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e487aa64126b47fc9bbf31662e298c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: UC_MatrixRL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f6d30358cb40cda4a0a0c7a5a18eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fde788708c499884c4cef039ad3f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def save():\n",
    "    '''\n",
    "    A function that saves the data for the a given experiment after each run\n",
    "    right now this saves the data under the DeepTree experimental data folder\n",
    "    '''\n",
    "    np.save(\"paper_vtr_data_deeptree/R_vtr\",R_vtr)\n",
    "    np.save(\"paper_vtr_data_deeptree/R_mat\",R_mat)\n",
    "    np.save(\"paper_vtr_data_deeptree/R_avg_eg_vtr\",R_avg_vtr)\n",
    "    np.save(\"paper_vtr_data_deeptree/R_avg_mat\",R_avg_mat)\n",
    "    np.save(\"paper_vtr_data_deeptree/R_eg_vtr\",R_eg)\n",
    "    np.save(\"paper_vtr_data_deeptree/R_mat_eg\",R_mat_eg)\n",
    "\n",
    "    np.save(\"paper_vtr_data_deeptree/model_vtr\",model_vtr)\n",
    "    np.save(\"paper_vtr_data_deeptree/model_mat\",model_mat)\n",
    "    np.save(\"paper_vtr_data_deeptree/model_eg_vtr\",model_eg_vtr)\n",
    "    np.save(\"paper_vtr_data_deeptree/model_eg_mat\",model_eg_mat)\n",
    "    \n",
    "    np.save(\"paper_vtr_data_deeptree/sem_vtr\",sem_vtr)\n",
    "    np.save(\"paper_vtr_data_deeptree/sem_mat\",sem_mat)\n",
    "\n",
    "\n",
    "#Environment Episode\n",
    "K = 10000\n",
    "#Controls how many different states on would like to test on \n",
    "Trials = 3\n",
    "#The number of Epsilon Greedy experiments to average over\n",
    "Runs = 5\n",
    "#The epsilon for the Eps Greedy methods\n",
    "eps = 0.1\n",
    "\n",
    "#Initialize Reward Vector for both UCRL-VTR and UC-MatrixRL\n",
    "R_vtr = np.zeros((Trials,K))\n",
    "R_mat = np.zeros((Trials,K))\n",
    "\n",
    "#Initialize Reward Matrix for both EGRL-VTR and EG-Freq\n",
    "R_eg = np.zeros((Trials,Runs,K))\n",
    "R_mat_eg = np.zeros((Trials,Runs,K))\n",
    "\n",
    "#A Matrix that stores the AVERAGE reward for both EGRL-VTR and EG-Freq\n",
    "R_avg_vtr = np.zeros((Trials,K))\n",
    "R_avg_mat = np.zeros((Trials,K))\n",
    "\n",
    "#The SEM of the reward for the Epsilon Greedy Methods\n",
    "sem_vtr = np.zeros((Trials,K))\n",
    "sem_mat = np.zeros((Trials,K))\n",
    "\n",
    "#Stores the Weighted L1 Error for all algorithms\n",
    "model_vtr = np.zeros((Trials,K))\n",
    "model_mat = np.zeros((Trials,K))\n",
    "model_eg_vtr = np.zeros((Trials,K))\n",
    "model_eg_mat = np.zeros((Trials,K))\n",
    "\n",
    "#Loops through multiple different states of a given environment\n",
    "for s in tqdm(range(Trials)):\n",
    "    \n",
    "    #Initialize the environment for testing\n",
    "    env = make_MDP(epLen = 2,nBottom = (s+1)*4)\n",
    "    #env = make_riverSwim(epLen=4*(s+3),nState = s+3)\n",
    "    \n",
    "    #Runs UCRL-VTR and stores Reward/Model Error\n",
    "    agent_vtr = UCRL_VTR(env,K)\n",
    "    R_vtr[s-3,:] = agent_vtr.run()\n",
    "    model_vtr[s-3,:] = agent_vtr.model_error\n",
    "    \n",
    "    #Runs UC-MatrixRL and stores Reward/Model Error\n",
    "    agent_mat = UC_MatrixRL(env,K)\n",
    "    R_mat[s-3,:] = agent_mat.run()\n",
    "    model_mat[s-3,:] = agent_mat.model_error\n",
    "    \n",
    "    #Initialize a vector that stores model error to be averaged for the Eps-Greedy Methods\n",
    "    model_err_avg_vtr = np.zeros(K)\n",
    "    model_err_avg_mat = np.zeros(K)\n",
    "    \n",
    "    #Loops through multiple runs of the Eps Greedy Algorithms\n",
    "    for run in tqdm(range(Runs)):\n",
    "        \n",
    "        #Runs EGRL-VTR and stores Reward/Model Error\n",
    "        agent_eg = EGRL_VTR(env,K,eps)\n",
    "        R_eg[s,run,:] = agent_eg.run()\n",
    "        model_err_avg_vtr += agent_eg.model_error\n",
    "        \n",
    "        #Runs EG-MatrixRL and stores Reward/Model Error\n",
    "        agent_mat_eg = EG_MatrixRL(env,K,eps)\n",
    "        R_mat_eg[s,run,:] = agent_mat_eg.run()\n",
    "        model_err_avg_mat += agent_mat_eg.model_error\n",
    "        \n",
    "    #Averages the Model Error for the Eps-Greedy methods    \n",
    "    model_eg_vtr[s,:] = model_err_avg_vtr/Runs\n",
    "    model_eg_mat[s,:] = model_err_avg_mat/Runs\n",
    "    \n",
    "    #Averages the Reward for the Eps-Greedy methods     \n",
    "    R_avg_vtr[s,:] = np.mean(R_eg[s,:,:],axis=0)\n",
    "    R_avg_mat[s,:] = np.mean(R_mat_eg[s,:,:],axis=0)\n",
    "    \n",
    "    #Computes the SEM for the Eps-Greedy methods\n",
    "    sem_vtr[s,:] = scipy.stats.sem(R_eg[s,:,:],axis=0)\n",
    "    sem_mat[s,:] = scipy.stats.sem(R_mat_eg[s,:,:],axis=0)\n",
    "    \n",
    "    #Saves the data at the end of each \"Trial\"\n",
    "    save()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import bernoulli\n",
    "#from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''General RL environment'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def advance(self, action):\n",
    "        '''\n",
    "        Moves one step in the environment.\n",
    "        Args:\n",
    "            action\n",
    "        Returns:\n",
    "            reward - double - reward\n",
    "            newState - int - new state\n",
    "            pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_riverSwim(epLen=20, nState=5):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "\n",
    "    for s in range(nState):\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[0, 0] = (5/1000, 0)\n",
    "    R_true[nState - 1, 1] = (1, 0)\n",
    "\n",
    "    # Transitions\n",
    "    for s in range(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "    for s in range(1, nState - 1):\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 0.3\n",
    "        P_true[s, 1][s] = 0.6\n",
    "        P_true[s, 1][max(0, s-1)] = 0.1\n",
    "\n",
    "    P_true[0, 1][0] = 0.3\n",
    "    P_true[0, 1][1] = 0.7\n",
    "    P_true[nState - 1, 1][nState - 1] = 0.9\n",
    "    P_true[nState - 1, 1][nState - 2] = 0.1\n",
    "\n",
    "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
    "    riverSwim.R = R_true\n",
    "    riverSwim.P = P_true\n",
    "    riverSwim.reset()\n",
    "\n",
    "    return riverSwim\n",
    "\n",
    "class TabularMDP(Environment):\n",
    "    '''\n",
    "    Tabular MDP\n",
    "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
    "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nState, nAction, epLen):\n",
    "        '''\n",
    "        Initialize a tabular episodic MDP\n",
    "        Args:\n",
    "            nState  - int - number of states\n",
    "            nAction - int - number of actions\n",
    "            epLen   - int - episode length\n",
    "        Returns:\n",
    "            Environment object\n",
    "        '''\n",
    "\n",
    "        self.nState = nState\n",
    "        self.nAction = nAction\n",
    "        self.epLen = epLen\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "        # Now initialize R and P\n",
    "        self.R = {}\n",
    "        self.P = {}\n",
    "        for state in range(nState):\n",
    "            for action in range(nAction):\n",
    "                self.R[state, action] = (1, 1)\n",
    "                self.P[state, action] = np.ones(nState) / nState\n",
    "                \n",
    "    def reset(self):\n",
    "        \"Resets the Environment\"\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "        \n",
    "    def advance(self,action):\n",
    "        '''\n",
    "        Move one step in the environment\n",
    "        Args:\n",
    "        action - int - chosen action\n",
    "        Returns:\n",
    "        reward - double - reward\n",
    "        newState - int - new state\n",
    "        episodeEnd - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        if self.R[self.state, action][1] < 1e-9:\n",
    "            # Hack for no noise\n",
    "            reward = self.R[self.state, action][0]\n",
    "        else:\n",
    "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
    "                                      scale=self.R[self.state, action][1])\n",
    "        #print(self.state, action, self.P[self.state, action])\n",
    "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
    "        \n",
    "        # Update the environment\n",
    "        self.state = newState\n",
    "        self.timestep += 1\n",
    "\n",
    "        episodeEnd = 0\n",
    "        if self.timestep == self.epLen:\n",
    "            episodeEnd = 1\n",
    "            #newState = None\n",
    "            self.reset()\n",
    "\n",
    "        return reward, newState, episodeEnd\n",
    "    \n",
    "    def argmax(self,b):\n",
    "        #print(b)\n",
    "        return np.random.choice(np.where(b == b.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(x, lo, hi):\n",
    "    '''Projects the value of x into the [lo,hi] interval'''\n",
    "    return max(min(x,hi),lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCRL_VTR(object):\n",
    "    '''\n",
    "    Algorithm 1 as described in the paper Model-Based RL with\n",
    "    Value-Target Regression\n",
    "    The algorithm assumes that the rewards are in the [0,1] interval.\n",
    "    '''\n",
    "    def __init__(self,env,K):\n",
    "        self.env = env\n",
    "        self.K = K\n",
    "        # Here the dimension (self.d) for the Tabular setting is |S x A x S| as stated in Appendix B\n",
    "        self.d = env.nState * env.nAction * env.nState \n",
    "        # In the tabular setting the basis models is just the dxd identity matrix, see Appendix B\n",
    "        self.P_basis = np.identity(self.d)\n",
    "        #Our Q-values are initialized as a 2d numpy array, will eventually convert to a dictionary\n",
    "        self.Q = [np.zeros((env.nState,env.nAction)) for i in range(env.epLen)]\n",
    "        #Our State Value function is initialized as a 1d numpy error, will eventually convert to a dictionary\n",
    "        self.V = [np.zeros(env.nState) for i in range(env.epLen+1)] # self.V[env.epLen] stays zero\n",
    "        #The index of each (s,a,s') tuple, see Appendix B\n",
    "        self.sigma = {}\n",
    "        self.createSigma()\n",
    "        #See Step 2, of algorithm 1\n",
    "#         self.M = env.epLen**2*self.d*np.identity(self.d)\n",
    "        self.lam = 1.0\n",
    "        self.L = 1.0\n",
    "        self.M = np.identity(self.d)*self.lam\n",
    "        #See Step 2\n",
    "        self.w = np.zeros(self.d)\n",
    "        #See Step 2\n",
    "        self.theta = np.matmul(np.linalg.inv(self.M),self.w)\n",
    "        #See Step 3\n",
    "        self.delta = 1/self.K\n",
    "        #C_theta >= the 2-norm of theta_star, see Assumption 1\n",
    "        self.C_theta = 3.0\n",
    "#         #Initialize the predicted value of the basis models, see equation 3\n",
    "#         self.X = np.zeros((env.epLen,self.d))\n",
    "\n",
    "    def feature_vector(self,s,a,h):\n",
    "        '''\n",
    "        Returning sum_{s'} V[h+1][s'] P_dot(s'|s,a),\n",
    "        with V stored in self.\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        sums = np.zeros(self.d)\n",
    "        for ss in range(env.nState):\n",
    "            sums += self.V[h+1][ss] * self.P_basis[self.sigma[(s,a,ss)]]\n",
    "        return sums\n",
    "            \n",
    "    def update_Q(self,s,a,k,h):\n",
    "        '''\n",
    "        A function that updates both Q and V, Q is updated according to equation 4 and \n",
    "        V is updated according to equation 2\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            k - the current episode\n",
    "            h - the current timestep within the episode\n",
    "        Currently, does not properly compute the Q-values but it does seem to learn theta_star\n",
    "        '''\n",
    "        #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "        # Alex's code: X = self.X[h,:] \n",
    "        # Suggested code:\n",
    "        X = self.feature_vector(s,a,h)\n",
    "        self.Q[h][s,a] = proj(env.R[(s,a)][0] + np.dot(X,self.theta) + self.Beta(k) \\\n",
    "            * np.sqrt(np.dot(np.dot(np.transpose(X),np.linalg.inv(self.M)),X)), 0, env.epLen )\n",
    "        self.V[h][s] = max(self.Q[h][s,:])\n",
    "    \n",
    "    def update_Qend(self,k):\n",
    "        '''\n",
    "        A function that updates both Q and V at the end of each episode, see step 16 of algorithm 1\n",
    "        Inputs:\n",
    "            k - the current episode\n",
    "        '''\n",
    "        #step 16\n",
    "        for h in range(env.epLen-1,-1,-1):\n",
    "            for s in range(env.nState):\n",
    "                for a in range(env.nAction):\n",
    "                    #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "                    # Alex's code: X = self.X[h,:] \n",
    "                    # Suggested code:\n",
    "                    self.update_Q(s,a,k,h)\n",
    "                self.V[h][s] = max(self.Q[h][s,:])\n",
    "    \n",
    "    def update_stat(self,s,a,s_,h):\n",
    "        '''\n",
    "        A function that performs steps 9-13 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            a - the action\n",
    "            s_ - the next state\n",
    "            k - the current episode\n",
    "            h - the timestep within episode when s was visited (starting at zero)\n",
    "        '''\n",
    "        #Step 10\n",
    "#         self.X[h,:] = self.feature_vector(s,a,h) # do not need to store this\n",
    "        X = self.feature_vector(s,a,h)\n",
    "        #Step 11\n",
    "        y = self.V[h+1][s_]\n",
    "#         if s_ != None:\n",
    "#             y = self.V[h+1][s_]\n",
    "#         else:\n",
    "#             y = 0.0\n",
    "        #Step 12\n",
    "        self.M = self.M + np.outer(X,X)\n",
    "        #Step 13\n",
    "        self.w = self.w + y*X\n",
    "    \n",
    "    def update_param(self):\n",
    "        '''\n",
    "        Updates our approximation of theta_star at the end of each episode, see \n",
    "        Step 15 of algorithm1\n",
    "        '''\n",
    "        #Step 15\n",
    "        #print(self.M)\n",
    "        self.theta = np.matmul(np.linalg.inv(self.M),self.w)\n",
    "        \n",
    "    def act(self,s,h):\n",
    "        '''\n",
    "        Returns the greedy action with respect to Q_{h,k}(s,a) for a \\in A\n",
    "        see step 8 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        #step 8\n",
    "        return env.argmax(self.Q[h][s,:])\n",
    "        # return bernoulli.rvs(0.9) #A random policy for testing\n",
    "        \n",
    "    def createSigma(self):\n",
    "        '''\n",
    "        A simple function that creates sigma according to Appendix B.\n",
    "        Here sigma is a dictionary who inputs is a tuple (s,a,s') and stores\n",
    "        the interger index to be used in our basis model P.\n",
    "        '''\n",
    "        i = 0\n",
    "        for s in range(env.nState):\n",
    "            for a in range(env.nAction):\n",
    "                for s_ in range(env.nState):\n",
    "                    self.sigma[(s,a,s_)] = int(i)\n",
    "                    i += 1\n",
    "    \n",
    "    def Beta(self,k):\n",
    "        '''\n",
    "        A function that return Beta_k according to Algorithm 1, step 3\n",
    "        '''\n",
    "        #Step 3\n",
    "        #Bonus as according to step 3\n",
    "        #return 16*pow(self.C_theta,2)*pow(env.epLen,2)*self.d*np.log(1+env.epLen*k) \\\n",
    "        #    *np.log(pow(k+1,2)*env.epLen/self.delta)*np.log(pow(k+1,2)*env.epLen/self.delta)\n",
    "        \n",
    "        #Confidence bound from Chapter 19/20 of the Bandit Algorithm book\n",
    "        first = np.sqrt(self.lam)*self.L\n",
    "        second = np.sqrt(2*np.log(1/self.delta) + self.d*np.log((self.d*self.lam + k*self.L*self.L)/(self.d*self.lam)))\n",
    "        return first + second\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_riverSwim(epLen = 20, nState = 4)\n",
    "K = 100\n",
    "agent = UCRL_VTR(env,K)\n",
    "count = np.zeros((env.nState,env.nState))\n",
    "R = 0\n",
    "for k in range(1,K+1):\n",
    "    env.reset()\n",
    "    done = 0\n",
    "    while done != 1:\n",
    "        s = env.state\n",
    "        h = env.timestep\n",
    "        a = agent.act(s,h)\n",
    "        r,s_,done = env.advance(a)\n",
    "        R += r\n",
    "        count[s,s_] += 1\n",
    "        agent.update_stat(s,a,s_,h)\n",
    "    agent.update_param()\n",
    "    agent.update_Qend(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-norm of (P_true - theta_star) is: 1.2643956538169365\n",
      "The total reward is 527.2599999999999\n"
     ]
    }
   ],
   "source": [
    "true_p = []\n",
    "for values in env.P.values():\n",
    "    for value in values:\n",
    "        true_p.append(value)\n",
    "print('The 2-norm of (P_true - theta_star) is:',np.linalg.norm(true_p-agent.theta))\n",
    "#When epLen = 20, k = 100, nState = 4, the cumlative reward of the old bonus was ~12, \n",
    "#with the bonus from the bandit algorithm book it s ~460, the optimal policy cumlative reward \n",
    "#with these parameters is ~800.\n",
    "print('The total reward is:', R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.84652486e-01,  1.53622165e-02, -4.48324771e-04,  4.05006054e-04,\n",
       "        4.28276410e-01,  5.69635994e-01,  9.61941613e-04,  2.99294562e-03,\n",
       "        9.69299478e-01,  3.42594882e-02, -6.36210085e-03,  2.65715458e-03,\n",
       "        3.16429077e-02,  7.35174128e-01,  2.24427534e-01,  1.04796513e-02,\n",
       "        1.08320004e-01,  8.79548571e-01,  1.76607034e-02, -5.53039191e-03,\n",
       "       -5.09963078e-01,  9.46590149e-01,  2.01348130e-01,  3.91325049e-01,\n",
       "        3.48400943e-03,  2.99804743e-04,  9.80665045e-01,  1.49720128e-02,\n",
       "        2.82735960e-01, -4.13678370e-01,  1.66713253e-01,  9.31155912e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.82404356 18.95583903]\n",
      " [19.23185577 19.51060409]\n",
      " [20.         20.        ]\n",
      " [20.         20.        ]]\n",
      "[[17.70953919 17.69966203]\n",
      " [18.17218124 18.55788825]\n",
      " [20.         20.        ]\n",
      " [20.         20.        ]]\n",
      "[[16.32746496 16.3141904 ]\n",
      " [16.89001908 17.19689512]\n",
      " [18.93947808 19.66207162]\n",
      " [20.         20.        ]]\n",
      "[[15.05366465 15.04519871]\n",
      " [15.663916   15.82523209]\n",
      " [17.41453721 18.58130794]\n",
      " [20.         20.        ]]\n",
      "[[13.80137454 13.85467144]\n",
      " [14.36223827 14.38483951]\n",
      " [15.73017725 17.20693048]\n",
      " [19.25613745 20.        ]]\n",
      "[[12.46116308 12.5132217 ]\n",
      " [12.97833483 12.99998327]\n",
      " [14.23568677 15.6096709 ]\n",
      " [17.47109707 19.19369111]]\n",
      "[[11.20865884 11.26136764]\n",
      " [11.68871872 11.71330642]\n",
      " [12.85499012 14.13327182]\n",
      " [15.82227312 17.48482788]]\n",
      "[[10.03395217 10.08689661]\n",
      " [10.48290682 10.51313818]\n",
      " [11.57502475 12.77067551]\n",
      " [14.30847827 15.90997454]]\n",
      "[[ 8.92496248  8.97835538]\n",
      " [ 9.34716152  9.38281959]\n",
      " [10.3741947  11.50747884]\n",
      " [12.91341198 14.46677476]]\n",
      "[[ 7.87340233  7.92174857]\n",
      " [ 8.26944606  8.30622405]\n",
      " [ 9.22861019 10.32141068]\n",
      " [11.61830245 13.14511923]]\n",
      "[[ 6.86810454  6.91336208]\n",
      " [ 7.23605251  7.26729547]\n",
      " [ 8.1149661   9.18569911]\n",
      " [10.37662797 11.9165322 ]]\n",
      "[[ 5.91427269  5.94356162]\n",
      " [ 6.24808743  6.26572726]\n",
      " [ 7.02970292  8.07872369]\n",
      " [ 9.17608876 10.74531962]]\n",
      "[[5.00599782 5.03237101]\n",
      " [5.30001439 5.29498245]\n",
      " [5.96134958 6.99149879]\n",
      " [7.96768398 9.58417745]]\n",
      "[[4.14422454 4.16357752]\n",
      " [4.39172436 4.35355793]\n",
      " [4.91356491 5.92142124]\n",
      " [6.7690813  8.43141378]]\n",
      "[[3.33032987 3.34243949]\n",
      " [3.52267751 3.43394113]\n",
      " [3.87968675 4.86342888]\n",
      " [5.56254818 7.27659839]]\n",
      "[[2.56160879 2.55895361]\n",
      " [2.69025063 2.52566223]\n",
      " [2.86516227 3.80332932]\n",
      " [4.33672702 6.11600265]]\n",
      "[[1.81459119 1.80838681]\n",
      " [1.88095972 1.63011513]\n",
      " [1.91655314 2.72415467]\n",
      " [3.06385164 4.92405572]]\n",
      "[[1.09234366 1.07389069]\n",
      " [1.11172667 0.80235977]\n",
      " [1.09378248 1.64383529]\n",
      " [1.82027735 3.65972178]]\n",
      "[[0.43081222 0.4510489 ]\n",
      " [0.4264552  0.18442001]\n",
      " [0.49034476 0.66618427]\n",
      " [0.69949923 2.32866496]]\n",
      "[[0.005 0.   ]\n",
      " [0.    0.   ]\n",
      " [0.    0.   ]\n",
      " [0.    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "for z in agent.Q:\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import bernoulli\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''General RL environment'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def advance(self, action):\n",
    "        '''\n",
    "        Moves one step in the environment.\n",
    "        Args:\n",
    "            action\n",
    "        Returns:\n",
    "            reward - double - reward\n",
    "            newState - int - new state\n",
    "            pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_riverSwim(epLen=20, nState=5):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "    states = {}\n",
    "    for s in range(nState):\n",
    "        states[(s)] = 0.0\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[0, 0] = (5/1000, 0)\n",
    "    R_true[nState - 1, 1] = (1, 0)\n",
    "\n",
    "    # Transitions\n",
    "    for s in range(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "    for s in range(1, nState - 1):\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 0.3\n",
    "        P_true[s, 1][s] = 0.6\n",
    "        P_true[s, 1][max(0, s-1)] = 0.1\n",
    "\n",
    "    P_true[0, 1][0] = 0.3\n",
    "    P_true[0, 1][1] = 0.7\n",
    "    P_true[nState - 1, 1][nState - 1] = 0.9\n",
    "    P_true[nState - 1, 1][nState - 2] = 0.1\n",
    "\n",
    "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
    "    riverSwim.R = R_true\n",
    "    riverSwim.P = P_true\n",
    "    riverSwim.states = states\n",
    "    riverSwim.reset()\n",
    "\n",
    "    return riverSwim\n",
    "\n",
    "class TabularMDP(Environment):\n",
    "    '''\n",
    "    Tabular MDP\n",
    "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
    "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nState, nAction, epLen):\n",
    "        '''\n",
    "        Initialize a tabular episodic MDP\n",
    "        Args:\n",
    "            nState  - int - number of states\n",
    "            nAction - int - number of actions\n",
    "            epLen   - int - episode length\n",
    "        Returns:\n",
    "            Environment object\n",
    "        '''\n",
    "\n",
    "        self.nState = nState\n",
    "        self.nAction = nAction\n",
    "        self.epLen = epLen\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "        # Now initialize R and P\n",
    "        self.R = {}\n",
    "        self.P = {}\n",
    "        self.states = {}\n",
    "        for state in range(nState):\n",
    "            for action in range(nAction):\n",
    "                self.R[state, action] = (1, 1)\n",
    "                self.P[state, action] = np.ones(nState) / nState\n",
    "                \n",
    "    def reset(self):\n",
    "        \"Resets the Environment\"\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "        \n",
    "    def advance(self,action):\n",
    "        '''\n",
    "        Move one step in the environment\n",
    "        Args:\n",
    "        action - int - chosen action\n",
    "        Returns:\n",
    "        reward - double - reward\n",
    "        newState - int - new state\n",
    "        episodeEnd - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        if self.R[self.state, action][1] < 1e-9:\n",
    "            # Hack for no noise\n",
    "            reward = self.R[self.state, action][0]\n",
    "        else:\n",
    "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
    "                                      scale=self.R[self.state, action][1])\n",
    "        #print(self.state, action, self.P[self.state, action])\n",
    "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
    "        \n",
    "        # Update the environment\n",
    "        self.state = newState\n",
    "        self.timestep += 1\n",
    "\n",
    "        episodeEnd = 0\n",
    "        if self.timestep == self.epLen:\n",
    "            episodeEnd = 1\n",
    "            #newState = None\n",
    "            self.reset()\n",
    "\n",
    "        return reward, newState, episodeEnd\n",
    "    \n",
    "    def argmax(self,b):\n",
    "        #print(b)\n",
    "        return np.random.choice(np.where(b == b.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deep_sea(Environment):\n",
    "    '''\n",
    "    Description:\n",
    "        A deep sea environment, where a diver goes\n",
    "        down and each time and she needs to make a\n",
    "        decision to go left or right.\n",
    "        environment terminates after fixed time step\n",
    "\n",
    "    Observation:\n",
    "        [horizontal position, vertical position]\n",
    "\n",
    "    Actions:\n",
    "        2 possible actions:\n",
    "        0 - left\n",
    "        1 - right\n",
    "\n",
    "    Starting State:\n",
    "        start at position 0, time step 0\n",
    "\n",
    "    Episode termination:\n",
    "        Env terminates after fixed number of time steps\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_steps):\n",
    "        '''\n",
    "        Inputs:\n",
    "            num_steps: An integer that represents the size of the DeepSea environment\n",
    "        '''\n",
    "        self.num_steps = num_steps\n",
    "        self.epLen = num_steps\n",
    "        self.flip_mask = 2*np.random.binomial(1,0.5,(num_steps,num_steps))-1\n",
    "        self.nAction = 2\n",
    "        self.nState = pow(num_steps,2)\n",
    "        self.epLen = num_steps\n",
    "        self.R = {}\n",
    "        self.states = {}\n",
    "        for s in range(self.num_steps):\n",
    "            for s_ in range(self.num_steps):\n",
    "                self.R[(s,s_), 0] = (0, 0)\n",
    "                self.R[(s,s_), 1] = (-0.01/self.nState, 0)\n",
    "                self.states[(s,s_)] = 0\n",
    "        self.R[(self.num_steps-1,self.num_steps-1),1] = (0.99,0)\n",
    "\n",
    "    def name(self):\n",
    "        return  \"deep sea\"\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0,0)\n",
    "        self.timestep = 0\n",
    "        return copy.deepcopy(self.state)\n",
    "\n",
    "    def advance(self,action):\n",
    "        assert action in [0,1], \"invalid action\"\n",
    "        self.state_prev = self.state\n",
    "        step_horizontal = (2*action-1)\n",
    "        horizontal = max(self.state[0] + step_horizontal, 0)\n",
    "        vertical = self.state[1] + 1\n",
    "        done =  bool(vertical == self.num_steps)\n",
    "        self.state = (horizontal, vertical)\n",
    "        self.timestep += 1\n",
    "        return self.R[self.state_prev,action][0], copy.deepcopy(self.state), done\n",
    "\n",
    "    def argmax(self,b):\n",
    "        return np.random.choice(np.where(b == b.max())[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(x, lo, hi):\n",
    "    '''Projects the value of x into the [lo,hi] interval'''\n",
    "    return max(min(x,hi),lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCRL_VTR(object):\n",
    "    '''\n",
    "    Algorithm 1 as described in the paper Model-Based RL with\n",
    "    Value-Target Regression\n",
    "    The algorithm assumes that the rewards are in the [0,1] interval.\n",
    "    '''\n",
    "    def __init__(self,env,K,random_explore):\n",
    "        self.env = env\n",
    "        self.K = K\n",
    "        # A unit test that randomly explores for a period of time then learns from that experience\n",
    "        # Here self.random_explore is a way to select a period of random exploration.\n",
    "        # When the current episode k > total number of episodes K divided by self.random_explore\n",
    "        # the algorithm switches to the greedy action with respect to its action value Q(s,a).\n",
    "        if random_explore:\n",
    "            self.random_explore = 10\n",
    "        else:\n",
    "            self.random_explore = self.K\n",
    "        # Here the dimension (self.d) for the Tabular setting is |S x A x S| as stated in Appendix B\n",
    "        self.d = self.env.nState * self.env.nAction * self.env.nState\n",
    "        # In the tabular setting the basis models is just the dxd identity matrix, see Appendix B\n",
    "        self.P_basis = np.identity(self.d)\n",
    "        #Our Q-values are initialized as a 2d numpy array, will eventually convert to a dictionary\n",
    "        self.Q = {(h,s,a): 0.0 for h in range(self.env.epLen) for s in self.env.states.keys() \\\n",
    "                   for a in range(self.env.nAction)}\n",
    "        #Our State Value function is initialized as a 1d numpy error, will eventually convert to a dictionary\n",
    "        self.V = {(h,s): 0.0 for s in self.env.states.keys() for h in range(env.epLen + 1)} # self.V[env.epLen] stays zero\n",
    "        #self.create_value_functions()\n",
    "        #The index of each (s,a,s') tuple, see Appendix B\n",
    "        self.sigma = {}\n",
    "        self.state_idx = {}\n",
    "        self.createIdx()\n",
    "        #See Step 2, of algorithm 1\n",
    "#         self.M = env.epLen**2*self.d*np.identity(self.d)\n",
    "        # For use in the confidence bound bonus term, see Beta function down below\n",
    "        self.lam = 1.0\n",
    "        #Self.L is no longer need, but will keep for now.\n",
    "        self.L = 1.0\n",
    "        self.M = np.identity(self.d)*self.lam\n",
    "        #For use in the Sherman-Morrison Update\n",
    "        self.Minv = np.identity(self.d)*(1/self.lam)\n",
    "        #See Step 2\n",
    "        self.w = np.zeros(self.d)\n",
    "        #See Step 2\n",
    "        self.theta = np.dot(self.Minv,self.w)\n",
    "        #See Step 3\n",
    "        self.delta = 1/self.K\n",
    "        #m_2 >= the 2-norm of theta_star, see Bandit Algorithms Theorem 20.5\n",
    "        self.error()\n",
    "        # See Theorem 20.5 for m_2\n",
    "        self.m_2 = np.linalg.norm(self.true_p) + 0.1\n",
    "#         #Initialize the predicted value of the basis models, see equation 3\n",
    "#         self.X = np.zeros((env.epLen,self.d))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def feature_vector(self,s,a,h):\n",
    "        '''\n",
    "        Returning sum_{s'} V[h+1][s'] P_dot(s'|s,a),\n",
    "        with V stored in self.\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        sums = np.zeros(self.d)\n",
    "        for s_ in self.env.states.keys():\n",
    "            #print(s,s_)\n",
    "            sums += self.V[h+1,s_] * self.P_basis[self.sigma[(s,a,s_)]]\n",
    "        return sums\n",
    "\n",
    "    def proj(self, x, lo, hi):\n",
    "        '''Projects the value of x into the [lo,hi] interval'''\n",
    "        return max(min(x,hi),lo)\n",
    "\n",
    "    def update_Q(self,s,a,k,h):\n",
    "        '''\n",
    "        A function that updates both Q and V, Q is updated according to equation 4 and\n",
    "        V is updated according to equation 2\n",
    "        Inputs:\n",
    "            s - the state\n",
    "            a - the action\n",
    "            k - the current episode\n",
    "            h - the current timestep within the episode\n",
    "        Currently, does not properly compute the Q-values but it does seem to learn theta_star\n",
    "        '''\n",
    "        #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "        # Alex's code: X = self.X[h,:]\n",
    "        # Suggested code:\n",
    "        X = self.feature_vector(s,a,h)\n",
    "        self.Q[h,s,a] = self.proj(self.env.R[(s,a)][0] + np.dot(X,self.theta) + self.Beta(k) \\\n",
    "            * np.sqrt(np.dot(np.dot(np.transpose(X),self.Minv),X)), 0, self.env.epLen )\n",
    "        self.V[h,s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def update_Qend(self,k):\n",
    "        '''\n",
    "        A function that updates both Q and V at the end of each episode, see step 16 of algorithm 1\n",
    "        Inputs:\n",
    "            k - the current episode\n",
    "        '''\n",
    "        #step 16\n",
    "        for h in range(self.env.epLen-1,-1,-1):\n",
    "            for s in self.env.states.keys():\n",
    "                for a in range(self.env.nAction):\n",
    "                    #Here env.R[(s,a)][0] is the true reward from the environment\n",
    "                    # Alex's code: X = self.X[h,:]\n",
    "                    # Suggested code:\n",
    "                    self.update_Q(s,a,k,h)\n",
    "                self.V[h,s] = max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "\n",
    "    def update_stat(self,s,a,s_,h):\n",
    "        '''\n",
    "        A function that performs steps 9-13 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            a - the action\n",
    "            s_ - the next state\n",
    "            k - the current episode\n",
    "            h - the timestep within episode when s was visited (starting at zero)\n",
    "        '''\n",
    "        #Step 10\n",
    "#         self.X[h,:] = self.feature_vector(s,a,h) # do not need to store this\n",
    "        X = self.feature_vector(s,a,h)\n",
    "        #Step 11\n",
    "        y = self.V[h+1,s_]\n",
    "#         if s_ != None:\n",
    "#             y = self.V[h+1][s_]\n",
    "#         else:\n",
    "#             y = 0.0\n",
    "        #Step 12\n",
    "        self.M = self.M + np.outer(X,X)\n",
    "        #Sherman-Morrison Update\n",
    "        self.Minv = self.Minv - np.dot((np.outer(np.dot(self.Minv,X),X)),self.Minv) / \\\n",
    "                    (1 + np.dot(np.dot(X,self.Minv),X))\n",
    "        #Step 13\n",
    "        self.w = self.w + y*X\n",
    "\n",
    "    def update_param(self):\n",
    "        '''\n",
    "        Updates our approximation of theta_star at the end of each episode, see\n",
    "        Step 15 of algorithm1\n",
    "        '''\n",
    "        #Step 15\n",
    "        #print(self.M)\n",
    "        self.theta = np.matmul(self.Minv,self.w)\n",
    "\n",
    "    def act(self,s,h,k):\n",
    "        '''\n",
    "        Returns the greedy action with respect to Q_{h,k}(s,a) for a \\in A\n",
    "        see step 8 of algorithm 1\n",
    "        Inputs:\n",
    "            s - the current state\n",
    "            h - the current timestep within the episode\n",
    "        '''\n",
    "        #step 8\n",
    "        if k > self.K /self.random_explore:\n",
    "            #print (max(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)])))\n",
    "            return self.env.argmax(np.array([self.Q[(h,s,a)] for a in range(self.env.nAction)]))\n",
    "        else:\n",
    "            return bernoulli.rvs(0.9) #A random policy for testing\n",
    "\n",
    "    def createIdx(self):\n",
    "        '''\n",
    "        A simple function that creates sigma according to Appendix B.\n",
    "        Here sigma is a dictionary who inputs is a tuple (s,a,s') and stores\n",
    "        the interger index to be used in our basis model P.\n",
    "        '''\n",
    "        i = 0\n",
    "        j = 0\n",
    "        k = 0\n",
    "        for s in self.env.states.keys():\n",
    "            self.state_idx[s] = int(j)\n",
    "            j += 1\n",
    "            for a in range(self.env.nAction):\n",
    "                for s_ in self.env.states.keys():\n",
    "                    self.sigma[(s,a,s_)] = int(i)\n",
    "                    i += 1\n",
    "\n",
    "    def Beta(self,k):\n",
    "        '''\n",
    "        A function that return Beta_k according to Algorithm 1, step 3\n",
    "        '''\n",
    "        #Step 3\n",
    "        #Bonus as according to step 3\n",
    "        #return 16*pow(self.m_2,2)*pow(env.epLen,2)*self.d*np.log(1+env.epLen*k) \\\n",
    "        #    *np.log(pow(k+1,2)*env.epLen/self.delta)*np.log(pow(k+1,2)*env.epLen/self.delta)\n",
    "\n",
    "        #Confidence bound from Chapter 20 of the Bandit Algorithms book, see Theorem 20.5.\n",
    "        first = np.sqrt(self.lam)*self.m_2\n",
    "        (sign, logdet) = np.linalg.slogdet(self.M)\n",
    "        #second = np.sqrt(2*np.log(1/self.delta) + self.d*np.log((self.d*self.lam + k*self.L*self.L)/(self.d*self.lam)))\n",
    "        det = sign * logdet\n",
    "        second = np.sqrt(2*np.log(1/self.delta) + np.log(k) + min(det,pow(10,10)) - np.log(pow(self.lam,self.d)))\n",
    "        return first + second\n",
    "\n",
    "    def run(self):\n",
    "        R = []\n",
    "        reward = 0.0\n",
    "        for k in tqdm(range(1,self.K+1)):\n",
    "            self.env.reset()\n",
    "            done = 0\n",
    "            while done != 1:\n",
    "                s = self.env.state\n",
    "                h = self.env.timestep\n",
    "                a = self.act(s,h,k)\n",
    "                r,s_,done = self.env.advance(a)\n",
    "                reward += r\n",
    "                self.update_stat(s,a,s_,h)\n",
    "            self.update_param()\n",
    "            self.update_Qend(k)\n",
    "            R.append(reward)\n",
    "        return R\n",
    "\n",
    "    def name(self):\n",
    "        return 'UCRL_VTR'\n",
    "    \n",
    "    def error(self):\n",
    "        self.true_p = []\n",
    "        for values in self.env.P.values():\n",
    "            for value in values:\n",
    "                self.true_p.append(value)\n",
    "        print('The 2-norm of (P_true - theta_star) is:',np.linalg.norm(self.true_p-self.theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_riverSwim(epLen = 50, nState = 10)\n",
    "K = 10000\n",
    "random_explore = False\n",
    "agent = UCRL_VTR(env,K,random_explore)\n",
    "R = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a198d9d392472b9b49f7a902e1821d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-norm of (P_true - theta_star) is: 3.88329756778952\n",
      "The 2-norm of (P_true - theta_star) is: 6.917441520337206\n",
      "The 2-norm of (P_true - theta_star) is: 6.318205576799904\n",
      "The 2-norm of (P_true - theta_star) is: 4.964800451730628\n",
      "The 2-norm of (P_true - theta_star) is: 5.068524176138277\n",
      "The 2-norm of (P_true - theta_star) is: 4.56423221392092\n",
      "The 2-norm of (P_true - theta_star) is: 5.360487063057704\n",
      "The 2-norm of (P_true - theta_star) is: 4.225445722476272\n",
      "The 2-norm of (P_true - theta_star) is: 3.6215547331527573\n",
      "The 2-norm of (P_true - theta_star) is: 3.601989952648023\n",
      "The 2-norm of (P_true - theta_star) is: 3.436201098321483\n",
      "The 2-norm of (P_true - theta_star) is: 3.068954786763823\n",
      "The 2-norm of (P_true - theta_star) is: 3.623821849053546\n",
      "The 2-norm of (P_true - theta_star) is: 3.3949829524358734\n",
      "The 2-norm of (P_true - theta_star) is: 3.5775683325899434\n",
      "The 2-norm of (P_true - theta_star) is: 4.513589247527173\n",
      "The 2-norm of (P_true - theta_star) is: 4.295381785991222\n",
      "The 2-norm of (P_true - theta_star) is: 4.811735299069386\n",
      "The 2-norm of (P_true - theta_star) is: 4.563526913603602\n",
      "The 2-norm of (P_true - theta_star) is: 4.866390522650895\n",
      "The 2-norm of (P_true - theta_star) is: 5.287552158037816\n",
      "The 2-norm of (P_true - theta_star) is: 5.545291453934577\n",
      "The 2-norm of (P_true - theta_star) is: 5.01548829847969\n",
      "The 2-norm of (P_true - theta_star) is: 3.5558010010121923\n",
      "The 2-norm of (P_true - theta_star) is: 4.801041168890346\n",
      "The 2-norm of (P_true - theta_star) is: 6.324394629296671\n",
      "The 2-norm of (P_true - theta_star) is: 6.859658643451064\n",
      "The 2-norm of (P_true - theta_star) is: 4.984816180650411\n",
      "The 2-norm of (P_true - theta_star) is: 3.5805905897335224\n",
      "The 2-norm of (P_true - theta_star) is: 2.97319300430371\n",
      "The 2-norm of (P_true - theta_star) is: 3.223784939015153\n",
      "The 2-norm of (P_true - theta_star) is: 5.092892528648538\n",
      "The 2-norm of (P_true - theta_star) is: 6.595361007271734\n",
      "The 2-norm of (P_true - theta_star) is: 8.220080617049753\n",
      "The 2-norm of (P_true - theta_star) is: 6.966345132347011\n",
      "The 2-norm of (P_true - theta_star) is: 6.3898638679147\n",
      "The 2-norm of (P_true - theta_star) is: 8.701467389809585\n",
      "The 2-norm of (P_true - theta_star) is: 8.54150464604073\n",
      "The 2-norm of (P_true - theta_star) is: 7.956026739599495\n",
      "The 2-norm of (P_true - theta_star) is: 10.814038201491098\n",
      "The 2-norm of (P_true - theta_star) is: 13.379209608569047\n",
      "The 2-norm of (P_true - theta_star) is: 11.227301450585019\n",
      "The 2-norm of (P_true - theta_star) is: 11.873514321118401\n",
      "The 2-norm of (P_true - theta_star) is: 12.393437496809641\n",
      "The 2-norm of (P_true - theta_star) is: 10.648706250806391\n",
      "The 2-norm of (P_true - theta_star) is: 10.741993521071421\n",
      "The 2-norm of (P_true - theta_star) is: 10.94424160390637\n",
      "The 2-norm of (P_true - theta_star) is: 10.469746049571047\n",
      "The 2-norm of (P_true - theta_star) is: 10.511223972280845\n",
      "The 2-norm of (P_true - theta_star) is: 10.571713662960919\n",
      "The 2-norm of (P_true - theta_star) is: 9.891253806165613\n",
      "The 2-norm of (P_true - theta_star) is: 9.777954703935889\n",
      "The 2-norm of (P_true - theta_star) is: 9.687602421368526\n",
      "The 2-norm of (P_true - theta_star) is: 9.355208791798262\n",
      "The 2-norm of (P_true - theta_star) is: 9.052835047793435\n",
      "The 2-norm of (P_true - theta_star) is: 8.579829199911543\n",
      "The 2-norm of (P_true - theta_star) is: 8.146281558817758\n",
      "The 2-norm of (P_true - theta_star) is: 7.781924897912832\n",
      "The 2-norm of (P_true - theta_star) is: 7.606023039362964\n",
      "The 2-norm of (P_true - theta_star) is: 7.7958013075199935\n",
      "The 2-norm of (P_true - theta_star) is: 7.480505360373326\n",
      "The 2-norm of (P_true - theta_star) is: 7.3848794024709985\n",
      "The 2-norm of (P_true - theta_star) is: 7.481774264360958\n",
      "The 2-norm of (P_true - theta_star) is: 7.358822190725356\n",
      "The 2-norm of (P_true - theta_star) is: 7.443248110792248\n",
      "The 2-norm of (P_true - theta_star) is: 7.325428908874937\n",
      "The 2-norm of (P_true - theta_star) is: 7.515742821567648\n",
      "The 2-norm of (P_true - theta_star) is: 7.245347964533527\n",
      "The 2-norm of (P_true - theta_star) is: 7.398184584116324\n",
      "The 2-norm of (P_true - theta_star) is: 7.766589747700892\n",
      "The 2-norm of (P_true - theta_star) is: 7.5540856215591745\n",
      "The 2-norm of (P_true - theta_star) is: 7.277008674194242\n",
      "The 2-norm of (P_true - theta_star) is: 7.3700182424772\n",
      "The 2-norm of (P_true - theta_star) is: 7.282568548361951\n",
      "The 2-norm of (P_true - theta_star) is: 7.166124382666331\n",
      "The 2-norm of (P_true - theta_star) is: 7.080230684621673\n",
      "The 2-norm of (P_true - theta_star) is: 6.936844412137669\n",
      "The 2-norm of (P_true - theta_star) is: 7.086920317872792\n",
      "The 2-norm of (P_true - theta_star) is: 7.095570710812583\n",
      "The 2-norm of (P_true - theta_star) is: 7.203248392546429\n",
      "The 2-norm of (P_true - theta_star) is: 7.1581185791158015\n",
      "The 2-norm of (P_true - theta_star) is: 7.049844659108584\n",
      "The 2-norm of (P_true - theta_star) is: 7.037294519036472\n",
      "The 2-norm of (P_true - theta_star) is: 7.03435722051906\n",
      "The 2-norm of (P_true - theta_star) is: 6.825506745125088\n",
      "The 2-norm of (P_true - theta_star) is: 6.988821436418512\n",
      "The 2-norm of (P_true - theta_star) is: 7.220113379028966\n",
      "The 2-norm of (P_true - theta_star) is: 7.155639467807729\n",
      "The 2-norm of (P_true - theta_star) is: 7.016083920414412\n",
      "The 2-norm of (P_true - theta_star) is: 6.844182323950239\n",
      "The 2-norm of (P_true - theta_star) is: 6.968916344927835\n",
      "The 2-norm of (P_true - theta_star) is: 7.035478765225876\n",
      "The 2-norm of (P_true - theta_star) is: 6.976201803828517\n",
      "The 2-norm of (P_true - theta_star) is: 7.055185099296402\n",
      "The 2-norm of (P_true - theta_star) is: 7.141068274906611\n",
      "The 2-norm of (P_true - theta_star) is: 7.134612477964106\n",
      "The 2-norm of (P_true - theta_star) is: 7.174268036255916\n",
      "The 2-norm of (P_true - theta_star) is: 7.340965754836617\n",
      "The 2-norm of (P_true - theta_star) is: 7.474936075839943\n",
      "The 2-norm of (P_true - theta_star) is: 7.336872259096686\n",
      "The 2-norm of (P_true - theta_star) is: 7.330259065109983\n",
      "The 2-norm of (P_true - theta_star) is: 7.360467544047577\n",
      "The 2-norm of (P_true - theta_star) is: 7.403540301589387\n",
      "The 2-norm of (P_true - theta_star) is: 7.164208779135359\n",
      "The 2-norm of (P_true - theta_star) is: 7.034229938903468\n",
      "The 2-norm of (P_true - theta_star) is: 6.931408035934408\n",
      "The 2-norm of (P_true - theta_star) is: 6.605718355715121\n",
      "The 2-norm of (P_true - theta_star) is: 6.494752444693289\n",
      "The 2-norm of (P_true - theta_star) is: 6.689442869064991\n",
      "The 2-norm of (P_true - theta_star) is: 6.731434564451973\n",
      "The 2-norm of (P_true - theta_star) is: 6.586091548331839\n",
      "The 2-norm of (P_true - theta_star) is: 6.451811978517941\n",
      "The 2-norm of (P_true - theta_star) is: 6.483864709270255\n",
      "The 2-norm of (P_true - theta_star) is: 6.350391961686619\n",
      "The 2-norm of (P_true - theta_star) is: 6.469068531900627\n",
      "The 2-norm of (P_true - theta_star) is: 6.335986831820522\n",
      "The 2-norm of (P_true - theta_star) is: 6.295533137829289\n",
      "The 2-norm of (P_true - theta_star) is: 6.371767349703289\n",
      "The 2-norm of (P_true - theta_star) is: 6.297829785462462\n",
      "The 2-norm of (P_true - theta_star) is: 6.508818644065453\n",
      "The 2-norm of (P_true - theta_star) is: 6.453415400452235\n",
      "The 2-norm of (P_true - theta_star) is: 6.411239958165122\n",
      "The 2-norm of (P_true - theta_star) is: 6.310066283678512\n",
      "The 2-norm of (P_true - theta_star) is: 6.181790181014077\n",
      "The 2-norm of (P_true - theta_star) is: 6.175539378753566\n",
      "The 2-norm of (P_true - theta_star) is: 6.118023693111626\n",
      "The 2-norm of (P_true - theta_star) is: 5.960618058709216\n",
      "The 2-norm of (P_true - theta_star) is: 6.018489044345574\n",
      "The 2-norm of (P_true - theta_star) is: 6.34335030267825\n",
      "The 2-norm of (P_true - theta_star) is: 6.389424175855969\n",
      "The 2-norm of (P_true - theta_star) is: 6.391094329471854\n",
      "The 2-norm of (P_true - theta_star) is: 6.130972316210833\n",
      "The 2-norm of (P_true - theta_star) is: 5.924776755344198\n",
      "The 2-norm of (P_true - theta_star) is: 5.973096307520367\n",
      "The 2-norm of (P_true - theta_star) is: 5.879874901707984\n",
      "The 2-norm of (P_true - theta_star) is: 5.912415282294794\n",
      "The 2-norm of (P_true - theta_star) is: 5.80663387939673\n",
      "The 2-norm of (P_true - theta_star) is: 5.783372476189876\n",
      "The 2-norm of (P_true - theta_star) is: 5.840668024622412\n",
      "The 2-norm of (P_true - theta_star) is: 5.774116978114918\n",
      "The 2-norm of (P_true - theta_star) is: 5.483457087263238\n",
      "The 2-norm of (P_true - theta_star) is: 5.673794726405353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-norm of (P_true - theta_star) is: 5.687487336478885\n",
      "The 2-norm of (P_true - theta_star) is: 5.555639075285011\n",
      "The 2-norm of (P_true - theta_star) is: 5.346388789398622\n",
      "The 2-norm of (P_true - theta_star) is: 5.449461614046362\n",
      "The 2-norm of (P_true - theta_star) is: 5.603647665331516\n",
      "The 2-norm of (P_true - theta_star) is: 5.769017047348451\n",
      "The 2-norm of (P_true - theta_star) is: 5.783468939647706\n",
      "The 2-norm of (P_true - theta_star) is: 5.715556772066636\n",
      "The 2-norm of (P_true - theta_star) is: 5.860627452220552\n",
      "The 2-norm of (P_true - theta_star) is: 5.930127208609516\n",
      "The 2-norm of (P_true - theta_star) is: 6.007319981262908\n",
      "The 2-norm of (P_true - theta_star) is: 6.166016640210974\n",
      "The 2-norm of (P_true - theta_star) is: 6.153411228731449\n",
      "The 2-norm of (P_true - theta_star) is: 6.240546087502805\n",
      "The 2-norm of (P_true - theta_star) is: 6.313577605289236\n",
      "The 2-norm of (P_true - theta_star) is: 6.414057208808914\n",
      "The 2-norm of (P_true - theta_star) is: 6.395277151545518\n",
      "The 2-norm of (P_true - theta_star) is: 6.355987452029756\n",
      "The 2-norm of (P_true - theta_star) is: 6.1596540795220545\n",
      "The 2-norm of (P_true - theta_star) is: 5.995521692997122\n",
      "The 2-norm of (P_true - theta_star) is: 6.004885451177285\n",
      "The 2-norm of (P_true - theta_star) is: 5.9888021037262416\n",
      "The 2-norm of (P_true - theta_star) is: 6.043207526285448\n",
      "The 2-norm of (P_true - theta_star) is: 6.046193509722876\n",
      "The 2-norm of (P_true - theta_star) is: 6.125947533573965\n",
      "The 2-norm of (P_true - theta_star) is: 6.171274773103674\n",
      "The 2-norm of (P_true - theta_star) is: 6.1435739995089875\n",
      "The 2-norm of (P_true - theta_star) is: 6.1093826680779895\n",
      "The 2-norm of (P_true - theta_star) is: 6.231364586434912\n",
      "The 2-norm of (P_true - theta_star) is: 6.183238306863889\n",
      "The 2-norm of (P_true - theta_star) is: 6.19115195206077\n",
      "The 2-norm of (P_true - theta_star) is: 6.264513049718374\n",
      "The 2-norm of (P_true - theta_star) is: 6.291434374186332\n",
      "The 2-norm of (P_true - theta_star) is: 6.236915352295079\n",
      "The 2-norm of (P_true - theta_star) is: 6.084979060091974\n",
      "The 2-norm of (P_true - theta_star) is: 5.789850046758212\n",
      "The 2-norm of (P_true - theta_star) is: 5.7216917608652285\n",
      "The 2-norm of (P_true - theta_star) is: 5.830215912618447\n",
      "The 2-norm of (P_true - theta_star) is: 5.675593922828905\n",
      "The 2-norm of (P_true - theta_star) is: 5.663162653050543\n",
      "The 2-norm of (P_true - theta_star) is: 5.372434962764391\n",
      "The 2-norm of (P_true - theta_star) is: 5.278671471138527\n",
      "The 2-norm of (P_true - theta_star) is: 5.283145017722534\n",
      "The 2-norm of (P_true - theta_star) is: 5.200813608661413\n",
      "The 2-norm of (P_true - theta_star) is: 5.064763798286486\n",
      "The 2-norm of (P_true - theta_star) is: 5.080840845299743\n",
      "The 2-norm of (P_true - theta_star) is: 5.128577209537019\n",
      "The 2-norm of (P_true - theta_star) is: 5.134281272502792\n",
      "The 2-norm of (P_true - theta_star) is: 5.125167467133681\n",
      "The 2-norm of (P_true - theta_star) is: 5.175101564914392\n",
      "The 2-norm of (P_true - theta_star) is: 5.168871127072555\n",
      "The 2-norm of (P_true - theta_star) is: 5.200343505745107\n",
      "The 2-norm of (P_true - theta_star) is: 5.165100916090316\n",
      "The 2-norm of (P_true - theta_star) is: 5.212215133571481\n",
      "The 2-norm of (P_true - theta_star) is: 5.224750884249958\n",
      "The 2-norm of (P_true - theta_star) is: 5.239783933780039\n",
      "The 2-norm of (P_true - theta_star) is: 5.150851914589217\n",
      "The 2-norm of (P_true - theta_star) is: 5.11934650909461\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env = deep_sea(num_steps = 4) It works on DeepSea, however, it is VERY SLOW in the Tabular Case.\n",
    "\n",
    "for k in tqdm(range(1,K+1)):\n",
    "    env.reset()\n",
    "    done = 0\n",
    "    while done != 1:\n",
    "        s = env.state\n",
    "        h = env.timestep\n",
    "        a = agent.act(s,h,k)\n",
    "        if k == K: print(a)\n",
    "        r,s_,done = env.advance(a)\n",
    "        R += r\n",
    "        #count[s,s_] += 1\n",
    "        if done != 1:\n",
    "            agent.update_stat(s,a,s_,h)\n",
    "    agent.update_param()\n",
    "    agent.update_Qend(k)\n",
    "    if k % 50 == 1:\n",
    "        agent.error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2-norm of (P_true - theta_star) is: 3.88329756778952\n",
      "The cumlative reward is: 60174.60999998527\n"
     ]
    }
   ],
   "source": [
    "true_p = []\n",
    "for values in env.P.values():\n",
    "    for value in values:\n",
    "        true_p.append(value)\n",
    "print('The 2-norm of (P_true - theta_star) is:',np.linalg.norm(true_p))\n",
    "\n",
    "#When epLen = 50, k = 1000, nState = 10, the cumlative reward  \n",
    "#with the bonus from the bandit algorithm book (Theorem 20.5) it is ~60,000, the optimal policy cumlative reward \n",
    "#with the above parameters is ~100,000.\n",
    "\n",
    "print('The cumlative reward is:', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99637558e-01,  3.92317075e-04, -6.27044392e-06, -3.18717248e-05,\n",
       "        7.41151689e-06,  1.57807517e-06, -4.16451257e-07, -2.33661467e-07,\n",
       "        2.91075594e-07, -4.73319176e-07,  3.83092864e-01,  5.97753136e-01,\n",
       "        1.31138546e-02,  1.69419180e-02, -1.69167862e-02,  8.44835092e-03,\n",
       "       -3.21719113e-03,  1.16449937e-03, -4.11211577e-04,  6.30831765e-05,\n",
       "        9.99484855e-01,  3.90144516e-04,  1.91863885e-04, -7.43639093e-05,\n",
       "        1.26406790e-05, -6.54328687e-07, -9.93196409e-07, -4.20814786e-07,\n",
       "       -3.32871809e-06, -1.54869895e-09,  2.06826966e-01,  4.32878452e-01,\n",
       "        3.95098210e-01, -5.02370844e-02,  1.95945989e-02, -8.75255321e-03,\n",
       "        8.60657121e-03, -3.76429996e-03, -1.63602504e-03,  9.18830757e-04,\n",
       "        1.45584722e-03,  9.98097144e-01,  3.81804534e-04,  5.41830921e-05,\n",
       "        2.50931788e-05, -2.85894521e-06, -1.03428710e-05,  7.97714176e-06,\n",
       "       -9.72147717e-06,  1.21216138e-06, -6.87549252e-02,  1.60784239e-01,\n",
       "        6.55933053e-01,  2.29778822e-01,  2.62343983e-02, -6.18171927e-03,\n",
       "        4.56609806e-03,  1.26225321e-03, -5.06650836e-03,  2.18052041e-03,\n",
       "        1.77255382e-04,  2.97980672e-04,  9.98996367e-01,  6.13556709e-04,\n",
       "       -8.51600846e-05,  2.76718640e-06, -1.26674431e-05,  3.92558125e-06,\n",
       "        1.38448952e-05, -4.97800571e-06, -8.30594914e-02,  7.48238295e-02,\n",
       "        1.25407564e-01,  5.79170201e-01,  3.01579686e-01,  1.21172346e-03,\n",
       "        4.61641435e-03, -8.68055735e-03,  5.77977092e-03, -9.51516361e-04,\n",
       "        7.81957779e-05, -1.16701596e-03,  2.20929346e-03,  9.98091908e-01,\n",
       "        7.89644470e-04,  5.59663234e-05,  5.44817155e-06, -2.81045750e-05,\n",
       "       -3.81531473e-05,  7.07089717e-06,  4.53597906e-01, -5.25969771e-01,\n",
       "        6.95830101e-02,  1.20904060e-01,  5.47736252e-01,  3.43082486e-01,\n",
       "       -3.79248896e-03, -1.43216360e-02,  1.34920179e-02, -5.68431477e-03,\n",
       "        1.33382499e-03, -1.19299677e-03, -7.76411033e-04,  1.38905992e-03,\n",
       "        9.98701549e-01,  6.22838858e-04, -4.60373680e-05, -4.10345304e-05,\n",
       "        2.54878520e-05, -3.70416871e-06,  3.30963022e-01, -5.20717725e-01,\n",
       "        2.84378490e-01, -7.77857928e-02,  5.35746390e-02,  6.45453075e-01,\n",
       "        2.70533113e-01,  2.67131433e-02, -4.72972180e-03, -7.05897751e-03,\n",
       "        3.38369304e-03, -3.70554207e-03,  4.91492552e-04, -7.34159913e-04,\n",
       "        1.47859813e-03,  9.98435179e-01,  6.54960207e-04,  1.03805611e-04,\n",
       "       -7.75853238e-05,  5.55208011e-06, -4.78113343e-02, -4.45049044e-01,\n",
       "        8.35383920e-01, -3.61675425e-01,  4.85232571e-03,  1.32804427e-01,\n",
       "        5.45596588e-01,  3.65357745e-01, -2.84771516e-02,  1.89710319e-05,\n",
       "       -3.54734138e-03,  3.79772348e-03, -1.50465801e-03,  4.92798563e-03,\n",
       "       -6.84646400e-03,  4.94838380e-03,  9.97729592e-01,  5.31975011e-04,\n",
       "        2.84717822e-06,  9.52685389e-06,  2.11332327e+00, -3.00423005e+00,\n",
       "        1.57145283e+00, -1.27781780e+00,  7.95347561e-01, -4.61080553e-01,\n",
       "        4.59589227e-01,  4.30927954e-01,  3.58814060e-01, -1.94908205e-02,\n",
       "        7.16751478e-04,  2.85434392e-03, -8.10706025e-03,  9.58142785e-03,\n",
       "       -4.68732610e-03, -8.36327558e-04,  2.42992746e-03,  9.97674763e-01,\n",
       "        1.05697758e-03, -2.03257637e-04,  5.96471337e-01, -1.08433844e+00,\n",
       "        1.24985305e+00, -1.01238723e+00,  3.74043607e-01,  5.61075033e-02,\n",
       "       -3.27347373e-01,  4.46719749e-01,  3.92101322e-01,  3.50467463e-01,\n",
       "       -2.50343099e-03,  4.31944049e-04,  3.91415650e-03, -3.83752064e-03,\n",
       "        2.56405545e-03,  2.37945289e-03, -6.89871395e-03,  6.27678615e-03,\n",
       "        9.96529782e-01,  1.12326848e-03, -1.84887704e-01,  2.00078639e-01,\n",
       "       -3.36781782e-02,  9.37354765e-02, -1.53561324e-01,  8.93131363e-02,\n",
       "       -3.38411469e-02,  4.78376475e-02,  5.16746721e-02,  9.18425263e-01])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''General RL environment'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def advance(self, action):\n",
    "        '''\n",
    "        Moves one step in the environment.\n",
    "        Args:\n",
    "            action\n",
    "        Returns:\n",
    "            reward - double - reward\n",
    "            newState - int - new state\n",
    "            pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_riverSwim(epLen=20, nState=5):\n",
    "    '''\n",
    "    Makes the benchmark RiverSwim MDP.\n",
    "    Args:\n",
    "        NULL - works for default implementation\n",
    "    Returns:\n",
    "        riverSwim - Tabular MDP environment '''\n",
    "    nAction = 2\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "\n",
    "    for s in range(nState):\n",
    "        for a in range(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards\n",
    "    R_true[0, 0] = (5 / 100, 0)\n",
    "    R_true[nState - 1, 1] = (1, 0)\n",
    "\n",
    "    # Transitions\n",
    "    for s in range(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "    for s in range(1, nState - 1):\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 0.3\n",
    "        P_true[s, 1][s] = 0.6\n",
    "        P_true[s, 1][max(0, s-1)] = 0.1\n",
    "\n",
    "    P_true[0, 1][0] = 0.3\n",
    "    P_true[0, 1][1] = 0.7\n",
    "    P_true[nState - 1, 1][nState - 1] = 0.9\n",
    "    P_true[nState - 1, 1][nState - 2] = 0.1\n",
    "\n",
    "    riverSwim = TabularMDP(nState, nAction, epLen)\n",
    "    riverSwim.R = R_true\n",
    "    riverSwim.P = P_true\n",
    "    riverSwim.reset()\n",
    "\n",
    "    return riverSwim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularMDP(Environment):\n",
    "    '''\n",
    "    Tabular MDP\n",
    "    R - dict by (s,a) - each R[s,a] = (meanReward, sdReward)\n",
    "    P - dict by (s,a) - each P[s,a] = transition vector size S\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nState, nAction, epLen):\n",
    "        '''\n",
    "        Initialize a tabular episodic MDP\n",
    "        Args:\n",
    "            nState  - int - number of states\n",
    "            nAction - int - number of actions\n",
    "            epLen   - int - episode length\n",
    "        Returns:\n",
    "            Environment object\n",
    "        '''\n",
    "\n",
    "        self.nState = nState\n",
    "        self.nAction = nAction\n",
    "        self.epLen = epLen\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "\n",
    "        # Now initialize R and P\n",
    "        self.R = {}\n",
    "        self.P = {}\n",
    "        for state in range(nState):\n",
    "            for action in range(nAction):\n",
    "                self.R[state, action] = (1, 1)\n",
    "                self.P[state, action] = np.ones(nState) / nState\n",
    "                \n",
    "    def reset(self):\n",
    "        \"Resets the Environment\"\n",
    "        self.timestep = 0\n",
    "        self.state = 0\n",
    "        \n",
    "    def advance(self,action):\n",
    "        '''\n",
    "        Move one step in the environment\n",
    "        Args:\n",
    "        action - int - chosen action\n",
    "        Returns:\n",
    "        reward - double - reward\n",
    "        newState - int - new state\n",
    "        pContinue - 0/1 - flag for end of the episode\n",
    "        '''\n",
    "        if self.R[self.state, action][1] < 1e-9:\n",
    "            # Hack for no noise\n",
    "            reward = self.R[self.state, action][0]\n",
    "        else:\n",
    "            reward = np.random.normal(loc=self.R[self.state, action][0],\n",
    "                                      scale=self.R[self.state, action][1])\n",
    "        #print(self.state, action, self.P[self.state, action])\n",
    "        newState = np.random.choice(self.nState, p=self.P[self.state, action])\n",
    "        \n",
    "        # Update the environment\n",
    "        self.state = newState\n",
    "        self.timestep += 1\n",
    "\n",
    "        if self.timestep == self.epLen:\n",
    "            pContinue = 1\n",
    "            #newState = None\n",
    "            self.reset()\n",
    "        else:\n",
    "            pContinue = 0\n",
    "\n",
    "        return reward, newState, pContinue\n",
    "    \n",
    "    def argmax(self,b):\n",
    "        return np.random.choice(np.where(b == b.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSRL(object):\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.alpha = {key: np.zeros(env.nState) for key in env.R.keys()} # need a more efficent method of creating this\n",
    "        self.mu = {key: 0.0 for key in env.R.keys()}\n",
    "        self.sigma2 = {key: 1.0 for key in env.R.keys()} #also this can be higher, no idea how initialization of this matrix effects learning, I assume higher initial values \"slows\" learning\n",
    "        self.sigma_r = 0.00001 #since our reward is deterministic, this keeps from the estimate of the reward from centering around the mean reward can this value be learned?\n",
    "        self.buffer = {h: [] for h in range(env.epLen)}\n",
    "        self.P = {key: np.zeros(env.nState) for key in env.R.keys()}\n",
    "        self.R = {key: 0.0 for key in env.R.keys()}\n",
    "        self.Rbar = {key: 0.0 for key in env.R.keys()}\n",
    "        self.Q = {key: 0.0 for key in env.R.keys()}\n",
    "    \n",
    "    def act(self,s):\n",
    "        x = np.array([self.Q[(s,a)] for a in range(self.env.nAction)])\n",
    "        return env.argmax(x)\n",
    "    \n",
    "    def learn(self,l):\n",
    "        self.update_statistics(l)\n",
    "        self.update_priors()\n",
    "        self.update_value_functions()\n",
    "    \n",
    "    def update_buffer(self,s,a,r,s_,t):\n",
    "        self.buffer[t].append((s,a,r,s_))\n",
    "    \n",
    "    def update_statistics(self,l):\n",
    "        for d in self.buffer.values():\n",
    "            #print(d)\n",
    "            s,a,r,s_ = d[l][0],d[l][1],d[l][2],d[l][3]\n",
    "            if s_ != None:\n",
    "                self.alpha[(s,a)][s_] = self.alpha[(s,a)][s_] + 1\n",
    "            self.mu[(s,a)] = (1/self.sigma2[(s,a)]*self.mu[(s,a)] + 1/self.sigma_r*r)/(1/self.sigma2[(s,a)] + 1/self.sigma_r)\n",
    "            self.sigma2[(s,a)] = 1 / (1/self.sigma2[(s,a)] + 1/self.sigma_r)\n",
    "    \n",
    "    def update_priors(self):\n",
    "        for s in range(env.nState):\n",
    "            for a in range(env.nAction):\n",
    "                self.Rbar[(s,a)] = np.random.normal(self.mu[(s,a)],self.sigma2[(s,a)])\n",
    "                self.R[(s,a)] = np.random.normal(self.Rbar[s,a],self.sigma_r)\n",
    "                self.P[(s,a)] = np.random.dirichlet(self.alpha[(s,a)] + 0.004) #since numpy's dirichlet doesn't take zeros add a small numerical value for stability\n",
    "\n",
    "    def update_value_functions(self):\n",
    "        Q = {key: 0.0 for key in env.R.keys()}\n",
    "        V = np.zeros(env.nState) #need to make this a dictionary\n",
    "        for _ in range(env.epLen+10): #why not? Seems to improve 'stability' to convergence of 'optimal' Q-values\n",
    "            for s in range(env.nState): \n",
    "                for a in range(env.nAction):\n",
    "                    w = np.random.normal(0,(pow(env.epLen+1,2))/(max(sum(self.alpha[s,a])-2,1)))\n",
    "                    Q[(s,a)] = self.R[(s,a)] + np.inner(self.P[(s,a)],V) + w\n",
    "                V[s] = max([Q[(s,a_)] for a_ in range(env.nAction)])\n",
    "        self.Q = Q.copy()\n",
    "        self.V = V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b845dd07a4d44e59c3dea583f58db51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = make_riverSwim(epLen = 20, nState = 5)\n",
    "agent = PSRL(env)\n",
    "for l in tqdm(range(1000)):\n",
    "    #env.reset()\n",
    "    done = 0\n",
    "    while done != 1:\n",
    "        s = env.state\n",
    "        a = agent.act(s)\n",
    "        t = env.timestep\n",
    "        r,s_,done = env.advance(a)\n",
    "        if done != 1:\n",
    "            agent.update_buffer(s,a,r,s_,t)\n",
    "        else:\n",
    "            agent.update_buffer(s,a,r,None,t)\n",
    "    agent.learn(l)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  0): array([1.00000000e+000, 2.12109355e-027, 1.09353152e-084, 0.00000000e+000,\n",
       "        8.94882877e-214]),\n",
       " (0, 1): array([3.17892148e-01, 6.82107852e-01, 3.93627235e-89, 8.71410467e-36,\n",
       "        2.06526630e-33]),\n",
       " (1,\n",
       "  0): array([1.00000000e+000, 2.25341636e-063, 9.37256278e-113, 7.17176768e-135,\n",
       "        2.63427800e-256]),\n",
       " (1, 1): array([1.23222287e-01, 5.71101634e-01, 3.05676079e-01, 1.54622397e-36,\n",
       "        4.94662910e-74]),\n",
       " (2,\n",
       "  0): array([1.21624958e-159, 1.00000000e+000, 3.83656776e-102, 1.11355439e-039,\n",
       "        2.13240020e-093]),\n",
       " (2,\n",
       "  1): array([0.00000000e+000, 9.63577666e-002, 5.78189437e-001, 3.25452796e-001,\n",
       "        5.60402689e-101]),\n",
       " (3,\n",
       "  0): array([1.48056261e-239, 1.62592634e-249, 1.00000000e+000, 1.52680536e-081,\n",
       "        7.94567871e-142]),\n",
       " (3, 1): array([6.82396734e-20, 1.43814186e-95, 9.95105924e-02, 6.03608659e-01,\n",
       "        2.96880749e-01]),\n",
       " (4,\n",
       "  0): array([6.83079699e-158, 2.03377595e-172, 1.84367720e-019, 1.00000000e+000,\n",
       "        1.54686903e-097]),\n",
       " (4, 1): array([7.61555243e-47, 1.11939142e-12, 2.15284901e-56, 8.66955578e-02,\n",
       "        9.13304442e-01])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for key in agent.P.keys():\n",
    "    arr.append(agent.P[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+000 2.12109355e-027 1.09353152e-084 0.00000000e+000\n",
      "  8.94882877e-214]\n",
      " [3.17892148e-001 6.82107852e-001 3.93627235e-089 8.71410467e-036\n",
      "  2.06526630e-033]\n",
      " [1.00000000e+000 2.25341636e-063 9.37256278e-113 7.17176768e-135\n",
      "  2.63427800e-256]\n",
      " [1.23222287e-001 5.71101634e-001 3.05676079e-001 1.54622397e-036\n",
      "  4.94662910e-074]\n",
      " [1.21624958e-159 1.00000000e+000 3.83656776e-102 1.11355439e-039\n",
      "  2.13240020e-093]\n",
      " [0.00000000e+000 9.63577666e-002 5.78189437e-001 3.25452796e-001\n",
      "  5.60402689e-101]\n",
      " [1.48056261e-239 1.62592634e-249 1.00000000e+000 1.52680536e-081\n",
      "  7.94567871e-142]\n",
      " [6.82396734e-020 1.43814186e-095 9.95105924e-002 6.03608659e-001\n",
      "  2.96880749e-001]\n",
      " [6.83079699e-158 2.03377595e-172 1.84367720e-019 1.00000000e+000\n",
      "  1.54686903e-097]\n",
      " [7.61555243e-047 1.11939142e-012 2.15284901e-056 8.66955578e-002\n",
      "  9.13304442e-001]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
